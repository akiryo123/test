# =============================================================
# Stage-C（JRA06専用）単勝配当予測モデル｜フルコード v2（最小修正版）
#  - 修正点:
#     1) scale_pos_weight 影響コードを完全削除
#     2) implied_win_odds / implied_place_odds / zone をdrop（FEATURESにも入らない）
#     3) 予測は raw ロジットをレース内 softmax で正規化（Train/Valid/Test 全て）
# =============================================================

print("--- Stage-C: 単勝的中率モデル パイプライン（JRA06専用）開始 ---")

# ------------------------
# Cell-0: Imports & Config
# ------------------------
import os, gc, json, re, pathlib, warnings
import numpy as np
import pandas as pd
import lightgbm as lgb
from datetime import datetime
warnings.filterwarnings("ignore")

SEED = 42
np.random.seed(SEED)
BET_UNIT = 100

# マージキー（FULLキー）
MERGE_KEYS = ["date", "race_code", "horse_number"]

# JRA06 パス定義（JRA05 禁止）
BASE_PARQUET = "/content/drive/My Drive/Colab/gpt/JRA06/JRA_base_data_2016.parquet"
SP_DERIVED   = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_time_index_S/JRA_derived_features_from_sp_model.parquet"
A_DERIVED    = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_a/JRA_derived_features_from_stage_a.parquet"
B_OOF        = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_b/JRA_stage_b_oof_predictions.parquet"
B_PRED       = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_b/JRA_stage_b_predictions.parquet"
ART_DIR_C    = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_c"
CAND_DERIVED = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_cand/JRA_derived_features_from_stage_cand.parquet"

STAGE_C_MODEL_PATH = os.path.join(ART_DIR_C, "JRA_stage_c_model.pkl")  # ← 追加

pathlib.Path(ART_DIR_C).mkdir(parents=True, exist_ok=True)

# カバレッジしきい値（未満なら停止）
COV_MIN_SP = 0.95
COV_MIN_A  = 0.95
# B は学習期間で OOF が欠損し得るので “any(OOF or pred)” で 0.80 以上を推奨
COV_MIN_B_ANY = 0.80

# 厳格リークガード（疑義が強い場合は停止）。無効化は環境変数で
STRICT_LEAK_GUARD = os.environ.get("STAGEC_STRICT_GUARD", "1") == "1"

# 使用する BASE 列（ユーザー指定）
BASE_COLS = [
    # --- レース基本情報 ---
    "race_code","date","month","race_number","venue_code","track_surface_code","track_code",
    "distance","meeting_order","day_order","race_type_code","race_condition_code","num_horses",
    "num_first_runners","num_front_runners","weight_type_code",
    # --- 馬の基本情報 ---
    "horse_number","sex_code","horse_age","horse_age_dec","career","weight_carried",
    "distance_change","rest_weeks","runs_since_layoff","stable_transfer_flag","place_rate",
    # --- オッズ・配当情報 ---
    "win_payout","trifecta_support_rate","pred_odds","pred_popularity","implied_win_odds",
    # --- 着順情報 ---
    "finishing_position","last_finish",
    # --- 予想/評価/指数 ---
    "pred_time_index","pred_dash_index","score","score_w","score_ver3","default_score",
    "pred_race_shape","fav_confidence",
    # --- 過去走情報 ---
    "last_field_size","last_venue_code","last_race_shape","last_final_3f_rank","last_jockey_rating",
    "last_popularity","last_pop_finish_diff","last_margin",
    # --- 関係者情報 ---
    "jockey_age","trainer_age","jockey_rank","trainer_rank",
    # --- 血統情報 ---
    "bloodline_score",
    # --- 差分/比率/統計 ---
    "pred_win_idx_diff","upset_index","upset_index2","career_gap","weight_ratio","last_field_size_diff",
    "fav_horse_rank_gap","stall_entry_order","pop_horse_score1","fav_horse_number_diff",
    "body_weight_category"  # ← zone は読み込まない
]

# ------------------------
# Cell-1: BASE 読込 & 型統一
# ------------------------
print("\n--- Cell-1: BASEデータ読込 & 列抽出 ---")
print(f"   - {BASE_PARQUET} を読み込みます…")
base = pd.read_parquet(BASE_PARQUET, engine="pyarrow")
exist = [c for c in BASE_COLS if c in base.columns]
miss  = [c for c in BASE_COLS if c not in base.columns]
base = base[exist].copy()
print(f"   - 指定列のうち存在: {len(exist)} / {len(BASE_COLS)}")
if miss:
    print(f"   - [WARN] 欠落列: {miss}")

# キー型統一（date: datetime, race_code/horse_number: Int64）
base["date"] = pd.to_datetime(base["date"])  # NaT 許容
for k in ["race_code","horse_number"]:
    if k in base.columns:
        base[k] = pd.to_numeric(base[k], errors="coerce").astype("Int64")
print("   - キー型統一: date=datetime64, race_code=Int64, horse_number=Int64")

# 事前に MAIN の列集合を保持（後段ガード用）
ALL_BASE_COLS = set(base.columns)

# ------------------------
# Cell-2: Stage-SP/A/B OOF & PRED を厳格マージ
# ------------------------
print("\n--- Cell-2: Stage-SP/A/B2/B の成果物を読み込みます ---")

# 型統一のユーティリティ
def _unify_keys(df: pd.DataFrame) -> pd.DataFrame:
    for k in MERGE_KEYS:
        assert k in df.columns, f"マージキー '{k}' がDataFrameに存在しません。"
    df["date"] = pd.to_datetime(df["date"])  # NaT 許容
    for k in ["race_code","horse_number"]:
        df[k] = pd.to_numeric(df[k], errors="coerce").astype("Int64")
    return df

# --- Stage-SP ---
sp_cols_allow = [
    # race内派生セット（リーク性なし）
    "pred_time_index_sp"
    #,"rank_pred_time_index_sp","mean_pred_time_index_sp_race",
    #"std_pred_time_index_sp_race","dev_pred_time_index_sp_race","top_pred_time_index_sp_race",
    #"diff_to_top_pred_time_index_sp",
]
try:
    sp = pd.read_parquet(SP_DERIVED, engine="pyarrow")
    sp = sp[[c for c in MERGE_KEYS + sp_cols_allow if c in sp.columns]].copy()
    sp = _unify_keys(sp)
    # キー重複は即停止
    if sp.duplicated(MERGE_KEYS).any():
        dup = int(sp.duplicated(MERGE_KEYS).sum())
        raise RuntimeError(f"Stage-SPのマージキーに重複があります（{dup}件）。上流を修正してください。")
    before = len(base)
    base = base.merge(sp, on=MERGE_KEYS, how="left")
    print(f"[MERGE] Stage-SP を LEFT JOIN します…\n   - 行数: after={len(base):,} (LEFT)")
except FileNotFoundError:
    raise RuntimeError(f"Stage-SP ファイルが見つかりません: {SP_DERIVED}")

# --- Stage-A ---
try:
    a = pd.read_parquet(A_DERIVED, engine="pyarrow")
    a_keep = [c for c in [*MERGE_KEYS, "pred_prob_stage_a"] if c in a.columns] #, "sp_gap","sp_percentile","sp_rank_in_race","prob_gap_vs_win"
    a = a[a_keep].copy()
    a = _unify_keys(a)
    if a.duplicated(MERGE_KEYS).any():
        dup = int(a.duplicated(MERGE_KEYS).sum())
        raise RuntimeError(f"Stage-Aのマージキーに重複があります（{dup}件）。上流を修正してください。")
    base = base.merge(a, on=MERGE_KEYS, how="left")
    print(f"[MERGE] Stage-A を LEFT JOIN します…\n   - 行数: after={len(base):,} (LEFT)")
except FileNotFoundError:
    raise RuntimeError(f"Stage-A ファイルが見つかりません: {A_DERIVED}")


# --- Stage-CAND（候補特徴群）---
cov_cand_any = 0.0
CAND_COLS_MERGED = []
try:
    cand = pd.read_parquet(CAND_DERIVED, engine="pyarrow")
    cand = _unify_keys(cand)

    # 1) 明確なリーク/不整合列を物理除外（存在しても学習に渡さない）
    #   - payout/finish/target系、学習時に不要な重み/フラグなどは落とす
    drop_exact = {
        "win_payout","place_payout","finishing_position",
        "target","target_raw","target_capped",
        "sample_weight_cand","exclude_flag_095","bottom3_by_cand",
        "time_index"  # 本パイプラインでは未使用
    }
    drop_regex = (r"^target_.*", r".*_payout$", r"^y_.*", r".*_label$")

    to_drop = set()
    for c in cand.columns:
        if c in MERGE_KEYS:
            continue
        if c in drop_exact:
            to_drop.add(c); continue
        if any(re.match(patt, c) for patt in drop_regex):
            to_drop.add(c)
    if to_drop:
        cand.drop(columns=list(to_drop), inplace=True, errors="ignore")

    # 2) 既に base にある列名との衝突は除外（キー以外は上書きしない方針）
    dup_cols = [c for c in cand.columns if (c in base.columns and c not in MERGE_KEYS)]
    if dup_cols:
        print(f"   - [INFO] Stage-CAND 既存列と重複のため除外: {dup_cols[:8]}{' ...' if len(dup_cols)>8 else ''}")
        cand.drop(columns=dup_cols, inplace=True, errors="ignore")

    # 3) マージキー重複チェック
    if cand.duplicated(MERGE_KEYS).any():
        dup = int(cand.duplicated(MERGE_KEYS).sum())
        raise RuntimeError(f"Stage-CANDのマージキーに重複があります（{dup}件）。上流を修正してください。")

    # 4) LEFT JOIN
    before = len(base)
    base = base.merge(cand, on=MERGE_KEYS, how="left")
    CAND_COLS_MERGED = [c for c in cand.columns if c not in MERGE_KEYS]
    print(f"[MERGE] Stage-CAND を LEFT JOIN します…\n   - 行数: after={len(base):,} (LEFT)")

    # 5) カバレッジ（any非欠損）を参考表示
    if CAND_COLS_MERGED:
        _nn = base[CAND_COLS_MERGED].notna().any(axis=1)
        cov_cand_any = float(_nn.mean())
        print(f"   - Stage-CAND 非欠損率(any): {cov_cand_any*100:.1f}%")

except FileNotFoundError:
    print(f"   - [WARN] Stage-CAND ファイルが見つかりません: {CAND_DERIVED}")



# --- Stage-B OOF（stage-b2 相当）---
try:
    b_oof = pd.read_parquet(B_OOF, engine="pyarrow")
    # 列正規化
    rename_map = {}
    if "pred_ev_stage_b" in b_oof.columns:
        rename_map["pred_ev_stage_b"] = "pred_ev_stage_b_oof"
    b_oof = b_oof.rename(columns=rename_map)
    b_oof = b_oof[[c for c in [*MERGE_KEYS, "pred_ev_stage_b_oof"] if c in b_oof.columns]].copy()
    b_oof = _unify_keys(b_oof)
    if b_oof.duplicated(MERGE_KEYS).any():
        dup = int(b_oof.duplicated(MERGE_KEYS).sum())
        raise RuntimeError(f"Stage-B OOF のマージキーに重複があります（{dup}件）。上流を修正してください。")
    base = base.merge(b_oof, on=MERGE_KEYS, how="left")
    print(f"[MERGE] Stage-B2(OOF) を LEFT JOIN します…\n   - 行数: after={len(base):,} (LEFT)")
except FileNotFoundError:
    print("   - [WARN] Stage-B2(OOF) が見つかりません。OOFは無しで進行します。")

# --- Stage-B pred（valid/test 用）---
try:
    b_pred = pd.read_parquet(B_PRED, engine="pyarrow")
    rename_map = {}
    if "pred_ev_stage_b" in b_pred.columns:
        rename_map["pred_ev_stage_b"] = "pred_ev_stage_b_pred"
    b_pred = b_pred.rename(columns=rename_map)
    keep = [c for c in [*MERGE_KEYS, "pred_ev_stage_b_pred", "split"] if c in b_pred.columns]
    b_pred = b_pred[keep].copy()
    b_pred = _unify_keys(b_pred)
    if b_pred.duplicated(MERGE_KEYS).any():
        dup = int(b_pred.duplicated(MERGE_KEYS).sum())
        raise RuntimeError(f"Stage-B(pred) のマージキーに重複があります（{dup}件）。上流を修正してください。")
    base = base.merge(b_pred, on=MERGE_KEYS, how="left")
    print(f"[MERGE] Stage-B(pred) を LEFT JOIN します…\n   - 行数: after={len(base):,} (LEFT)")
except FileNotFoundError:
    print("   - [WARN] Stage-B(pred) が見つかりません。predは無しで進行します。")

# any 列作成（OOF優先）
if "pred_ev_stage_b_oof" in base.columns or "pred_ev_stage_b_pred" in base.columns:
    base["pred_ev_stage_b_any"] = base["pred_ev_stage_b_oof"].fillna(base.get("pred_ev_stage_b_pred"))
else:
    base["pred_ev_stage_b_any"] = np.nan

# カバレッジ確認
probe = {
    "SP:pred_time_index_sp": base["pred_time_index_sp"].notna().mean() if "pred_time_index_sp" in base.columns else 0.0,
    "A:pred_prob_stage_a":    base["pred_prob_stage_a"].notna().mean() if "pred_prob_stage_a" in base.columns else 0.0,
    "B:any":                   base["pred_ev_stage_b_any"].notna().mean() if "pred_ev_stage_b_any" in base.columns else 0.0,
    "CAND:any":                cov_cand_any  # ← これを追加
}
print("\n[INFO] 'pred_ev_stage_b_any' を作成（OOF優先→pred）")
for k,v in probe.items():
    print(f"   - {k} 非欠損率: {v*100:.1f}%")

# --- PATCH A: Stage-Bの入力列をANYに完全統一（恒久対応） ---
B_DUP_COLS = ["pred_ev_stage_b_oof", "pred_ev_stage_b_pred"]
_drop_exists = [c for c in B_DUP_COLS if c in base.columns]
if _drop_exists:
    base.drop(columns=_drop_exists, inplace=True, errors="ignore")
    print(f"   - [CLEANUP] 学習契約を pred_ev_stage_b_any に統一するため、{_drop_exists} を削除しました。")


# 厳格停止条件
if probe.get("SP:pred_time_index_sp",0) < COV_MIN_SP:
    raise RuntimeError(f"Stage-SP カバー率不足: {probe['SP:pred_time_index_sp']*100:.1f}% < {COV_MIN_SP*100:.0f}%")
if probe.get("A:pred_prob_stage_a",0) < COV_MIN_A:
    raise RuntimeError(f"Stage-A カバー率不足: {probe['A:pred_prob_stage_a']*100:.1f}% < {COV_MIN_A*100:.0f}%")
# B は any ベースで緩め（足切りは警告のみ）
if probe.get("B:any",0) < COV_MIN_B_ANY:
    print(f"   - [WARN] Stage-B(any) カバー率が低いです: {probe['B:any']*100:.1f}% < {COV_MIN_B_ANY*100:.0f}%")



print("--- Cell-2: マージ完了 ---")

# --------------------------------------------------
# Cell-3: 時系列分割（開始・終了を明示）
# --------------------------------------------------
print("\n[INFO] 時系列に基づきデータを3つに分割します…")
base_sorted = base.sort_values("date").reset_index(drop=True)
# 後段のチェックで参照するために保持
df_all_for_guard = base_sorted.copy()

# 学習・評価期間（JRA用）
#TRAIN_START_DATE = pd.to_datetime("2016-01-01")
TRAIN_START_DATE = pd.to_datetime("2022-01-01") #短縮版！！！！！！！！！！！！！！！！
VALID_START_DATE = pd.to_datetime("2023-04-01")
VALID_END_DATE   = pd.to_datetime("2024-03-31")
TEST_START_DATE  = pd.to_datetime("2024-04-01")
TEST_END_DATE    = pd.to_datetime("2025-08-31")
TRAIN_END_DATE   = VALID_START_DATE - pd.Timedelta(days=1)

train = base_sorted[(base_sorted["date"] >= TRAIN_START_DATE) & (base_sorted["date"] <= TRAIN_END_DATE)].copy()
valid = base_sorted[(base_sorted["date"] >= VALID_START_DATE) & (base_sorted["date"] <= VALID_END_DATE)].copy()
test  = base_sorted[(base_sorted["date"] >= TEST_START_DATE)  & (base_sorted["date"] <= TEST_END_DATE)].copy()

# メモリ解放
del base, base_sorted; gc.collect()

if train.empty or valid.empty or test.empty:
    raise RuntimeError("データ分割後、いずれかのセットが空になりました。日付範囲を確認してください。")

print(f"   - 学習期間:   {TRAIN_START_DATE.date()} 〜 {TRAIN_END_DATE.date()}")
print(f"   - 検証期間:   {VALID_START_DATE.date()} 〜 {VALID_END_DATE.date()}")
print(f"   - テスト期間: {TEST_START_DATE.date()} 〜 {TEST_END_DATE.date()}")
print(f"   - Train={len(train):,}  Valid={len(valid):,}  Test={len(test):,}")

# セット別 B(any) カバー率
for name, df_ in (("Train", train), ("Valid", valid), ("Test", test)):
    cov = df_["pred_ev_stage_b_any"].notna().mean() if "pred_ev_stage_b_any" in df_.columns else 0.0
    print(f"   - [{name}] Stage-B(any) 非欠損率: {cov*100:.1f}%")

# --------------------------------------------------
# Cell-4: 目的変数（単勝） & 前処理
# --------------------------------------------------

NA_TOKEN = "__NA__"
RARE_TOKEN = "___RARE___"

def _norm_code_str(s: pd.Series) -> pd.Series:
    s = s.astype("string").str.strip()
    s = s.mask(s.str.len().fillna(0) == 0, NA_TOKEN)
    s = s.mask(s.str.lower().isin({"nan","none"}), NA_TOKEN)
    s = s.mask(s.isin({"0","0.0"}), NA_TOKEN)  # ← 初戦(0/0.0)を __NA__ へ
    return s

def _build_cats_with_rare_from_train(s_train: pd.Series, *, min_count=10, min_frac=0.0005, always_allow=None):
    s_tr = _norm_code_str(s_train)
    vc = s_tr.value_counts(dropna=False)

    thr = max(min_count, int(np.ceil(min_frac * max(1, len(s_tr)))))
    keep = set(vc[vc >= thr].index.tolist())
    if always_allow:
        keep |= {str(x) for x in always_allow}

    # 予約トークン整理
    keep.discard(RARE_TOKEN); keep.add(NA_TOKEN)
    cats = sorted(keep | {RARE_TOKEN})
    return cats, keep  # cats=最終categories, keep=RAREに畳まない集合(NA含む)

def _apply_categorical_with_rare(dfs, col, cats, keep):
    for df in dfs:
        s = _norm_code_str(df[col])
        s = s.where(s.isin(keep), RARE_TOKEN)
        df[col] = pd.Categorical(s, categories=cats)


print("\n--- Cell-4: 目的変数 & 前処理 ---")

# 目的変数（単勝的中=1）
for _df in (train, valid, test):
    _df["y_win"] = (_df["finishing_position"] == 1).astype("int8")

# 単純なカテゴリ整形（zone は除外済のため残っていればカテゴリ化しない）
cat_cols = [
    c for c in [
        "venue_code","track_surface_code","track_code","race_type_code","race_condition_code",
        "sex_code","weight_type_code","body_weight_category",
        "grade_code","last_venue_code","weekday_code","po_horse_flag","anagusa_flag","blinker_change_flag",
          # ← zone は含めない
    ] if c in df_all_for_guard.columns
]

# レア統合ポリシー
RARE_WHITELIST = {"last_venue_code"}  # ← 必ず畳み込み対象
RARE_MIN_COUNT = 10
RARE_MIN_FRAC  = 0.0005

for c in cat_cols:
    if c not in df_all_for_guard.columns:
        continue
    if str(train[c].dtype) not in ("category","object","string"):
        # 数値でも“コード”列はまず文字へ
        train[c] = train[c].astype("string")
        valid[c] = valid[c].astype("string")
        test[c]  = test[c].astype("string")

    # 列の固有数に応じて自動判定：多値(>30) or 明示ホワイトリストなら RARE 統合
    nuniq = _norm_code_str(train[c]).nunique(dropna=False)
    allow_rare = (c in RARE_WHITELIST) or (nuniq > 30)

    if allow_rare:
        cats, keep = _build_cats_with_rare_from_train(
            train[c], min_count=RARE_MIN_COUNT, min_frac=RARE_MIN_FRAC
        )
        _apply_categorical_with_rare([train, valid, test], c, cats, keep)
    else:
        # 小カーデ列は全値を保持（ただし NA/0→__NA__ は統一）
        s_tr = _norm_code_str(train[c])
        cats = sorted(set(s_tr.unique().tolist()) | {NA_TOKEN})  # RARE は敢えて入れない
        for df_ in (train, valid, test):
            s = _norm_code_str(df_[c])
            s = s.where(s.isin(cats), NA_TOKEN)  # 未知は欠損扱い
            df_[c] = pd.Categorical(s, categories=cats)

train = train.reset_index(drop=True)
valid = valid.reset_index(drop=True)
test  = test.reset_index(drop=True)


# 0/1 フラグ整形（object→0/1）
flag_cols = [c for c in ["stable_transfer_flag"] if c in df_all_for_guard.columns]
for c in flag_cols:
    for _df in (train, valid, test):
        s = _df[c]
        if s.dtype == "bool":
            _df[c] = s.astype("int8")
        else:
            _df[c] = s.astype("string").str.lower().map({"1":1,"true":1,"t":1,"y":1,"yes":1,"on":1,
                                                           "0":0,"false":0,"f":0,"n":0,"no":0,"off":0}).fillna(0).astype("int8")

# レース内派生特徴量（rank / z-score / top-gap）
RACE_GROUP = ["date","race_code"]

# === New: 季節・距離の簡易特徴 ===
def _season(m):
    if m in (3,4,5):   return "Spring"
    if m in (6,7,8):   return "Summer"
    if m in (9,10,11): return "Autumn"
    return "Winter"

for _df in (train, valid, test):
    # season（month→カテゴリ）
    _df["season"] = _df["month"].map(_season).astype("string").fillna("__NA__").astype("category")
    # 400mビン（例: 1200, 1400, 1600 ...）→ 文字カテゴリ化して一致性を担保
    _df["distance_bin_400"] = ((_df["distance"] // 400) * 400).astype("Int64").astype("string").fillna("__NA__").astype("category")
    # 距離クラス（ざっくり3分割）
    _df["distance_class3"] = pd.cut(
        _df["distance"].astype(float),
        bins=[0, 1400, 2000, 10000],
        labels=["Short","Middle","Long"],
        include_lowest=True
    ).astype("category")


# ★ Stage-C: 頭数binの付与（split後すぐ or feature確定直前）
def add_field_size_bin4(df, src_col="num_horses", new_col="num_horses_bin4"):
    bins   = [0, 8, 12, 16, 99]                 # 右閉
    labels = ["<=8", "9-12", "13-16", "17+"]
    df[new_col] = pd.cut(pd.to_numeric(df[src_col], errors="coerce"),
                         bins=bins, labels=labels, right=True, include_lowest=True).astype("category")

for _df in (train, valid, test):
    add_field_size_bin4(_df)

# 特徴量リストへ追加（名前差吸収）
cat_name = "num_horses_bin4"
if "feature_cols" in globals():
    feature_cols = list(dict.fromkeys(feature_cols + [cat_name]))
if "FEATURES" in globals():
    FEATURES = list(dict.fromkeys(FEATURES + [cat_name]))

# カテゴリ列へ追加（存在する変数名に揃える）
if "cat_cols_final" in globals():
    cat_cols_final = sorted(list(set(list(cat_cols_final) + [cat_name])))
elif "cat_cols" in globals():
    cat_cols = sorted(list(set(list(cat_cols) + [cat_name])))
elif "categorical_cols" in globals():
    categorical_cols = sorted(list(set(list(categorical_cols) + [cat_name])))


# LGBMに渡す追加カテゴリ列（後で cat_cols_final に結合）
extra_cat_cols = ["season","distance_bin_400","distance_class3"]


race_feats_src = [
    # 予想/指数系（存在すれば）
    "pred_time_index","pred_dash_index","score","score_w","score_ver3","default_score",
    # Stage-SP
    "pred_time_index_sp",
    #"rank_pred_time_index_sp","mean_pred_time_index_sp_race",
    #"std_pred_time_index_sp_race","dev_pred_time_index_sp_race","top_pred_time_index_sp_race",
    #"diff_to_top_pred_time_index_sp",
    # Stage-A / B
    "pred_prob_stage_a",
    #"sp_gap","sp_percentile","sp_rank_in_race",
    "pred_ev_stage_b_any"  # ← ここに追加
]


for col in race_feats_src:
    for _df in (train, valid, test):
        if col not in _df.columns:
            continue
        s = _df[col].astype(float)
        # rank（大きいほど良い前提）
        _df[f"r_{col}"] = s.groupby(_df[RACE_GROUP].apply(tuple, axis=1)) \
                           .rank(ascending=False, method="first").astype("float32")
        # z-score（レース内）
        g = _df.groupby(RACE_GROUP)[col]
        mu = g.transform("mean").astype("float32")
        sd = g.transform("std").astype("float32").replace(0, np.nan)
        _df[f"z_{col}"] = ((s - mu) / sd).astype("float32").fillna(0.0)
        # top-gap（上位との差）
        top = g.transform("max").astype("float32")
        _df[f"gap_{col}"] = (top - s).astype("float32")


# ==============================================================================
# タイトル: TSR正規化 & r_/z_ ギャップ（A↔TSR, SP↔TSR）
# 説明: 市場(TSR)をレース内確率(tsr_prob)に正規化し、A/SPとの順位差・z差を特徴量化します。
#       ・tsr_prob … trifecta_support_rate をレース内で合計=1に正規化
#       ・r_tsr_prob / z_tsr_prob … tsr_prob のレース内順位/標準化スコア
#       ・r_gap_a_tsr, z_gap_a_tsr … A(確率) vs TSR の順位差/標準化差
#       ・r_gap_sp_tsr, z_gap_sp_tsr … SP(指数) vs TSR の順位差/標準化差
#       ※ “r_*” は「数字が小さいほど強い（1位が最良）」ことに注意。符号は学習が吸収します。
# ==============================================================================
print("\n[INFO] TSR正規化 & r_/z_ギャップ（A↔TSR, SP↔TSR）を作成します…")
_added_cols = set()

for _df in (train, valid, test):
    if "trifecta_support_rate" not in _df.columns:
        continue

    # --- 1) tsr_prob: レース内合計=1 に正規化 --------------------------------
    gkey = _df[["date","race_code"]].apply(tuple, axis=1)
    s = pd.to_numeric(_df["trifecta_support_rate"], errors="coerce").fillna(0.0).astype("float32")
    denom = s.groupby(gkey).transform("sum").replace(0, np.nan)
    _df["tsr_prob"] = (s / denom).fillna(0.0).astype("float32"); _added_cols.add("tsr_prob")

    # --- 2) tsr_prob の r_/z_ -------------------------------------------------
    sp = _df["tsr_prob"].astype(float)
    _df["r_tsr_prob"] = sp.groupby(gkey).rank(ascending=False, method="first").astype("float32"); _added_cols.add("r_tsr_prob")

    mu = _df.groupby(["date","race_code"])["tsr_prob"].transform("mean").astype("float32")
    sd = _df.groupby(["date","race_code"])["tsr_prob"].transform("std").astype("float32").replace(0, np.nan)
    _df["z_tsr_prob"] = ((sp - mu) / sd).astype("float32").fillna(0.0); _added_cols.add("z_tsr_prob")

    # --- 3) A vs TSR のギャップ（r/z） ----------------------------------------
    if "r_pred_prob_stage_a" in _df.columns:
        _df["r_gap_a_tsr"] = (_df["r_pred_prob_stage_a"].astype("float32") - _df["r_tsr_prob"]).astype("float32")
        _added_cols.add("r_gap_a_tsr")
    if "z_pred_prob_stage_a" in _df.columns:
        _df["z_gap_a_tsr"] = (_df["z_pred_prob_stage_a"].astype("float32") - _df["z_tsr_prob"]).astype("float32")
        _added_cols.add("z_gap_a_tsr")

    # --- 4) SP vs TSR のギャップ（r/z） ---------------------------------------
    if "r_pred_time_index_sp" in _df.columns:
        _df["r_gap_sp_tsr"] = (_df["r_pred_time_index_sp"].astype("float32") - _df["r_tsr_prob"]).astype("float32")
        _added_cols.add("r_gap_sp_tsr")
    if "z_pred_time_index_sp" in _df.columns:
        _df["z_gap_sp_tsr"] = (_df["z_pred_time_index_sp"].astype("float32") - _df["z_tsr_prob"]).astype("float32")
        _added_cols.add("z_gap_sp_tsr")

print(f"   - 作成列: {sorted(_added_cols)}")
# ==============================================================================


# === New: r_*（レース内順位）群からの総合順位平均 & 偏差値 ===
def _keep_r_col(c: str) -> bool:
    """r_* のうち ‘二重順位/派生順位’ を除外して集計対象だけを残す"""
    if not c.startswith("r_"):
        return False
    # rank/mean/std/dev/top/diff/gap からの再ランキングは除外
    bad_prefixes = (
        "r_rank_", "r_mean_", "r_std_", "r_dev_", "r_top_", "r_diff_", "r_gap_",
        "r_sp_rank_in_race"  # 念のため
    )
    return not any(c.startswith(b) for b in bad_prefixes)

r_cols_all = [c for c in train.columns if _keep_r_col(c)]
if r_cols_all:
    for _df in (train, valid, test):
        n_in_race = _df.groupby(["date","race_code"])["horse_number"] \
                       .transform("count").astype("float32")
        R = _df[r_cols_all].astype("float32")
        R_rev = (n_in_race.values.reshape(-1,1) + 1.0) - R.values  # 1位→n に反転
        r_mean_all = np.nanmean(R_rev, axis=1).astype("float32")
        _df["r_mean_all"] = r_mean_all

        g = _df.groupby(["date","race_code"])["r_mean_all"]
        mu = g.transform("mean").astype("float32")
        sd = g.transform("std").astype("float32").replace(0, np.nan)
        _df["dev_r_mean_all"] = (50.0 + 10.0 * (_df["r_mean_all"] - mu) / sd) \
                                  .astype("float32").fillna(50.0)
else:
    print("   - [NOTE] r_* が見つからなかったため、総合順位特徴はスキップしました。")


# 置き換え：FEATURES を「分割後・派生作成後」の列から作る
ALL_COLS_UNION = sorted(set(train.columns) | set(valid.columns) | set(test.columns))

# 使わない/リーク列
LEAK_DROP = [
    "win_payout","finishing_position",
#    "trifecta_support_rate",
    "implied_place_odds","implied_win_odds","zone",
    "y_win","split"  # 目的変数
]

FEATURES = [c for c in ALL_COLS_UNION if c not in set(LEAK_DROP + MERGE_KEYS + ["race_number","date"])]


# ==============================================================================
# === アイデア1-A：AIモデル間の相互作用特徴量（A×B, Aランク−SPランク） ===
#   挿入位置：FEATURES を最終決定する直前
# ==============================================================================
print("\n[INFO] 相互作用特徴量(A×B, A-rank vs SP-rank) を作成します...")
_interact_added = []

for _df in (train, valid, test):
    # 1) 「Aの確率」×「Bの期待値」
    if "pred_prob_stage_a" in _df.columns and "pred_ev_stage_b_any" in _df.columns:
        _df["interact_Aprob_x_Bev"] = (
            _df["pred_prob_stage_a"].fillna(0.0).astype("float32")
            * _df["pred_ev_stage_b_any"].fillna(0.0).astype("float32")
        ).astype("float32")
        _interact_added.append("interact_Aprob_x_Bev")

    # 2) 「Aの確率ランク」−「SPの能力ランク」
    #   ※ r_* は「順位（1が最良）」なので、符号の解釈に注意
    if "r_pred_prob_stage_a" in _df.columns and "r_pred_time_index_sp" in _df.columns:
        _df["interact_Arank_vs_SPrank"] = (
            _df["r_pred_prob_stage_a"].astype("float32")
            - _df["r_pred_time_index_sp"].astype("float32")
        ).astype("float32")
        _interact_added.append("interact_Arank_vs_SPrank")

print(f"   - 新規作成: {sorted(set(_interact_added))}")
# ==============================================================================



# --- PATCH B: “万一”残っていても採用しない（恒久対応の二重安全弁） ---
DROP_B_DUP = {"pred_ev_stage_b_oof","pred_ev_stage_b_pred"}
if any(c in FEATURES for c in DROP_B_DUP):
    FEATURES = [c for c in FEATURES if c not in DROP_B_DUP]
    print("   - [CLEANUP] FEATURES から {pred_ev_stage_b_oof, pred_ev_stage_b_pred} を除外（ANYのみ使用）")

# LightGBM に渡すカテゴリ列は FEATURES との共通部分に限定
cat_cols_final = [c for c in (cat_cols + extra_cat_cols) if c in FEATURES]

# object型でカテゴリ指定されていないものは念のため除外
obj_bad = [c for c in FEATURES
           if (str(train[c].dtype) == "object") and (c not in cat_cols_final)]
if obj_bad:
    FEATURES = [c for c in FEATURES if c not in obj_bad]
    print(f"   - [NOTE] object型かつ未カテゴリの列を除外: {obj_bad[:6]}{' ...' if len(obj_bad)>6 else ''}")

print(f"   - v2 特徴量 数: {len(FEATURES)}  （カテゴリ列: {len(cat_cols_final)}）")

# Cell-4 のカテゴリ処理が終わった直後に追加
CAT_MAP_FINAL = {}      # 各列の最終 categories（順序含む）
KEEP_MAP_FINAL = {}     # RARE へ畳まない値集合（= categories から RARE を除いた集合）

for c in cat_cols:  # ← あなたのコードで使っているカテゴリ列リスト
    if c in train.columns and hasattr(train[c].dtype, "categories"):
        cats = list(train[c].cat.categories.astype("string"))
        CAT_MAP_FINAL[c] = cats
        KEEP_MAP_FINAL[c] = set(cats) - {"___RARE___"}  # RARE 以外は “保持”


# ------------------------
# Cell-4.5: 健全性ガード（エラー停止）
# ------------------------
print("\n[GUARD] 単勝評価の健全性チェックを開始します…")
EVAL_PAYOUT_COL = "win_payout"  # 評価は単勝のみ
if EVAL_PAYOUT_COL not in ALL_BASE_COLS:
    raise RuntimeError(f"[FATAL] 評価列 '{EVAL_PAYOUT_COL}' が見つかりません。BASE_PARQUET を確認してください。")

# 直近のリーク一次検査：FEATURES にリーク候補が混入していないか
suspicious = [c for c in ["win_payout","finishing_position"] if c in FEATURES] #,"trifecta_support_rate"
if suspicious:
    raise RuntimeError(f"[FATAL] リーク候補列が学習特徴量に含まれています: {suspicious}")


# ────────────────────────────────────────────────
# Cell-4.5 : Soft-Finish OOF特徴の作成（Stage-C 用）
#   s = 1, 0.25, 0.15, 0.05(4-5着), 0 を TimeSeriesSplit で回帰学習（時系列リーク対策）
#   string dtypes を category に統一して LightGBM に渡す（型不一致対策）
#   出力: train/valid/test すべてに 'feat_soft_finish' を付与し、feature_cols に追加
# ────────────────────────────────────────────────
print("\n" + "="*50)
print("◎ Soft-Finish OOF特徴の作成（Stage-C）")
print("="*50)

import os, json, gc
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit

# === Winsorize 設定（Train OOF の分布で閾値を決定） =====================
WINSOR_Q_LOW  = float(os.environ.get("SF_WINSOR_Q_LOW",  0.005))  # 下位0.5%
WINSOR_Q_HIGH = float(os.environ.get("SF_WINSOR_Q_HIGH", 0.995))  # 上位99.5%
assert 0.0 <= WINSOR_Q_LOW < WINSOR_Q_HIGH <= 1.0

# === 既存メタを拾う（環境差吸収） ==================================
def _pick_feature_cols():
    for k in ('feature_cols', 'FEATURE_COLS', 'FEATURES'):
        if k in globals(): return list(globals()[k])
    raise RuntimeError("[FATAL] feature_cols/FEATURE_COLS/FEATURES が見つかりません。")

def _pick_cat_cols():
    for k in ('categorical_cols', 'cat_cols', 'CAT_COLS', 'cat_cols_final'):
        if k in globals(): return list(globals()[k])
    # フォールバック：型から推定
    cols = _pick_feature_cols()
    return [c for c in cols if str(train[c].dtype) in ("category","object","string")]

SEED = globals().get('SEED', 42)
FEAT_COLS_ALL = _pick_feature_cols()
CAT_COLS_RAW  = _pick_cat_cols()

# 明示的に除外（アウトカムや配当などのリーク源が紛れた場合の安全弁）
LEAK_LIKE = {'finishing_position','finish_position','win_payout','place_payout','y_win','y_place','result_time'}
FEAT_COLS_SF = [c for c in FEAT_COLS_ALL if c not in LEAK_LIKE]


# （差し替え前の CAT_MAP_FINAL / KEEP_MAP_FINAL ブロックを削除して、これに置換）
USE_RARE = {}        # 列ごとに RARE_TOKEN を使っているか（True/False）
CAT_MAP_FINAL = {}   # 各列の最終 categories（順序含む）
KEEP_MAP_FINAL = {}  # RARE に畳まない値集合（= categories から RARE を除いた集合）

for c in cat_cols_final:  # ← cat_cols ではなく cat_cols_final を使う
    if c in train.columns and hasattr(train[c].dtype, "categories"):
        cats = list(train[c].cat.categories.astype("string"))
        CAT_MAP_FINAL[c] = cats
        has_rare = (RARE_TOKEN in cats)
        USE_RARE[c] = has_rare
        KEEP_MAP_FINAL[c] = set(cats) - ({RARE_TOKEN} if has_rare else set())

def _apply_categorical_template(dfs, col):
    """学習時に確定した categories/挙動（未知→RARE or 未知→NA）を強制適用"""
    cats = CAT_MAP_FINAL[col]
    keep = KEEP_MAP_FINAL[col]
    has_rare = USE_RARE[col]
    for df in dfs:
        s = _norm_code_str(df[col])
        s = s.where(s.isin(keep), (RARE_TOKEN if has_rare else NA_TOKEN))
        df[col] = pd.Categorical(s, categories=cats)

# --- ここを追加 ---------------------------------------------------------
# Soft-Finish で使う候補カテゴリ列（CAT_COLS_RAW）から、
# 実際に学習時に categories を確定させた列（CAT_MAP_FINAL にある列）のみ採用
cat_used_sf = [c for c in CAT_COLS_RAW if c in CAT_MAP_FINAL]

# 学習時と同じカテゴリ集合＆未知の扱い（RARE/NA）を Train/Valid/Test 全てに強制適用
for c in cat_used_sf:
    _apply_categorical_template([train, valid, test], c)
# ------------------------------------------------------------------------


# === Soft-Finish のターゲット s を作成 =================================
def _soft_finish_target(df):
    pos = pd.to_numeric(df.get('finishing_position'), errors='coerce').fillna(0).astype(int)
    s = np.zeros(len(df), dtype='float32')
    s[pos == 1] = 1.00
    s[pos == 2] = 0.25
    s[pos == 3] = 0.15
    s[(pos == 4) | (pos == 5)] = 0.05
    return s

y_sf = _soft_finish_target(train)

# --- ここから追加（自己参照/存在チェックガード）-------------------------
# 1) feat_soft_finish（これから作る特徴）は学習入力から必ず除外
EXCLUDE_SF_COLS = {'feat_soft_finish'}
FEAT_COLS_SF = [c for c in FEAT_COLS_SF if c not in EXCLUDE_SF_COLS]

# 2) Soft-Finish でも B 列は ANY に統一（まず重複列を除去し、ANYを先に組み込む）
B_REPLACE = {"pred_ev_stage_b_oof", "pred_ev_stage_b_pred"}
FEAT_COLS_SF = [c for c in FEAT_COLS_SF if c not in B_REPLACE]
# “any” を使えるなら採用（まずは train にあるかチェック）
if "pred_ev_stage_b_any" in train.columns and "pred_ev_stage_b_any" not in FEAT_COLS_SF:
    FEAT_COLS_SF.append("pred_ev_stage_b_any")
    print("[SF] Soft-Finish入力に 'pred_ev_stage_b_any' を追加しました。")

# 3) 列存在チェック（train→valid/test の順で共通集合へ）
#    ついでに順序維持＆重複排除もしておく
FEAT_COLS_SF = [c for c in FEAT_COLS_SF if c in train.columns]
_common = [c for c in FEAT_COLS_SF if (c in valid.columns and c in test.columns)]
if len(_common) < len(FEAT_COLS_SF):
    drop_n = len(FEAT_COLS_SF) - len(_common)
    print(f"[WARN] Soft-Finish: valid/test に無い列を {drop_n} 個削除しました。")
FEAT_COLS_SF = list(dict.fromkeys(_common))  # 順序維持で重複排除

# 4) categorical_feature も最終決定列に合わせて同期
cat_used_sf = [c for c in cat_used_sf if c in FEAT_COLS_SF]
# ----------------------------------------------------------------------


# === OOF 学習（TimeSeriesSplit で時系列リーク防止） ====================
sf_n_splits = int(os.environ.get("SF_OOF_K", 5))
print(f"[INFO] Soft-Finish OOF を作成します: n_splits={sf_n_splits} / splitter=TimeSeriesSplit（将来データ未混入）")
sf_splitter = TimeSeriesSplit(n_splits=sf_n_splits)

oof_sf = np.zeros(len(train), dtype="float32")
best_iters = []

params_sf = {
    "objective": "regression_l2",
    "metric": "rmse",
    "first_metric_only": True,
    "boosting_type": "gbdt",
    "learning_rate": 0.05,
    "num_leaves": 127,
    "min_data_in_leaf": 80,
    "feature_fraction": 0.85,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "lambda_l2": 1.0,
    "max_bin": 127,
    "force_col_wise": True,
    "two_round": True,
    "seed": SEED,
    "feature_fraction_seed": SEED,
    "bagging_seed": SEED,
    "data_random_seed": SEED,
    "verbosity": -1,
    "num_threads": -1,
}

# === Soft-Finish: Optuna best params で上書き（最小差分）================
SF_OPTUNA_BEST_PARAMS = {
    "learning_rate": 0.022337835932350347,
    "num_leaves": 36,
    "min_data_in_leaf": 150,
    "feature_fraction": 0.726764747776563,
    "bagging_fraction": 0.9011487502306803,
    "bagging_freq": 6,
    "lambda_l1": 5.572042886691552,
    "lambda_l2": 3.741982796639425,
    "min_gain_to_split": 0.006857114218896185,
    "feature_fraction_bynode": 0.7486419238599451  # v3+ で有効
}
_before = {k: params_sf.get(k) for k in SF_OPTUNA_BEST_PARAMS.keys()}
params_sf.update(SF_OPTUNA_BEST_PARAMS)
_after  = {k: params_sf.get(k) for k in SF_OPTUNA_BEST_PARAMS.keys()}
print("[SF-OPTUNA] Soft-Finish params overridden (before -> after):")
for k in SF_OPTUNA_BEST_PARAMS.keys():
    print(f"   - {k}: {_before[k]} -> {_after[k]}")

for fold, (tr_idx, va_idx) in enumerate(sf_splitter.split(train), 1):
    # ログ（学習の最大日付 < 検証の最小日付 を目視確認できるように）
    try:
        d_tr_max = pd.to_datetime(train.iloc[tr_idx]['date']).max()
        d_va_min = pd.to_datetime(train.iloc[va_idx]['date']).min()
        d_va_max = pd.to_datetime(train.iloc[va_idx]['date']).max()
        print(f"  - Fold {fold}/{sf_n_splits} | train<=[{d_tr_max.date()}] -> valid[{d_va_min.date()}~{d_va_max.date()}]")
    except Exception:
        print(f"  - Fold {fold}/{sf_n_splits}")

    X_tr = train.iloc[tr_idx][FEAT_COLS_SF]
    X_va = train.iloc[va_idx][FEAT_COLS_SF]
    y_tr = y_sf[tr_idx]
    y_va = y_sf[va_idx]

    d_tr = lgb.Dataset(
        X_tr, label=y_tr, categorical_feature=cat_used_sf,
        params={"max_bin": params_sf["max_bin"], "two_round": params_sf["two_round"], "data_random_seed": params_sf["data_random_seed"]},
        free_raw_data=False
    )
    d_va = lgb.Dataset(
        X_va, label=y_va, categorical_feature=cat_used_sf,
        reference=d_tr,
        params={"max_bin": params_sf["max_bin"], "two_round": params_sf["two_round"], "data_random_seed": params_sf["data_random_seed"]},
        free_raw_data=False
    )

    model_sf = lgb.train(
        params=params_sf,
        train_set=d_tr,
        valid_sets=[d_va],
        valid_names=["valid"],
        num_boost_round=3000,
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),
                   lgb.log_evaluation(period=0)]
    )

    oof_sf[va_idx] = model_sf.predict(X_va, num_iteration=model_sf.best_iteration)
    best_iters.append(int(model_sf.best_iteration or 100))

    # 明示解放（RAM抑制）
    del d_tr, d_va, X_tr, X_va, y_tr, y_va, model_sf
    gc.collect()

print(f"[INFO] OOF完成（soft-finish）: train.shape={oof_sf.shape}, valid/test は全学習モデルで推論します")
print(f"      best_iter (median/mean) = {int(np.median(best_iters))} / {float(np.mean(best_iters)):.1f}")

# === Train 全体で最終学習 → Valid/Test 推論 ============================
num_boost_round_sf = int(np.median(best_iters))
d_all = lgb.Dataset(
    train[FEAT_COLS_SF], label=y_sf, categorical_feature=cat_used_sf,
    params={"max_bin": params_sf["max_bin"], "two_round": params_sf["two_round"], "data_random_seed": params_sf["data_random_seed"]},
    free_raw_data=False
)
final_sf = lgb.train(params=params_sf, train_set=d_all, num_boost_round=num_boost_round_sf)

# 予測の付与（← の前に Winsorize を挟む）
# --- Train OOF 分布で上下閾値を決定 ---
lo, hi = np.quantile(oof_sf, [WINSOR_Q_LOW, WINSOR_Q_HIGH])
if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:
    # フォールバック：分布が極端な場合は、最小/最大の微小拡張に退避
    lo = float(np.nanmin(oof_sf))
    hi = float(np.nanmax(oof_sf))
    eps = max(1e-6, 1e-3 * (hi - lo))
    lo, hi = lo - eps, hi + eps
print(f"[WINSOR] Soft-Finish caps from OOF: q{WINSOR_Q_LOW:.3f}={lo:.4f}, q{WINSOR_Q_HIGH:.3f}={hi:.4f}")

# --- Valid/Test 用の生推論を先に作る ---
_pred_va_sf = final_sf.predict(valid[FEAT_COLS_SF], num_iteration=num_boost_round_sf).astype("float32")
_pred_te_sf = final_sf.predict(test[FEAT_COLS_SF],  num_iteration=num_boost_round_sf).astype("float32")

# --- クリップ適用（外れ抑制）---
oof_sf_w   = np.clip(oof_sf,    lo, hi).astype("float32")
pred_va_sf = np.clip(_pred_va_sf, lo, hi).astype("float32")
pred_te_sf = np.clip(_pred_te_sf, lo, hi).astype("float32")

# 付与
train['feat_soft_finish'] = oof_sf_w
valid['feat_soft_finish'] = pred_va_sf
test['feat_soft_finish']  = pred_te_sf

# 参考統計（winsorize 後）
def _q(a): return np.quantile(a, [0, .5, .9, .99, 1.0])
qt_tr = _q(train['feat_soft_finish'].values)
qt_va = _q(valid['feat_soft_finish'].values)
qt_te = _q(test['feat_soft_finish'].values)
print(f"[STAT] (winsor) feat_soft_finish Train  min/p50/p90/p99/max = {qt_tr[0]:.3f}/{qt_tr[1]:.3f}/{qt_tr[2]:.3f}/{qt_tr[3]:.3f}/{qt_tr[4]:.3f}")
print(f"[STAT] (winsor) feat_soft_finish Valid  min/p50/p90/p99/max = {qt_va[0]:.3f}/{qt_va[1]:.3f}/{qt_va[2]:.3f}/{qt_va[3]:.3f}/{qt_va[4]:.3f}")
print(f"[STAT] (winsor) feat_soft_finish Test   min/p50/p90/p99/max = {qt_te[0]:.3f}/{qt_te[1]:.3f}/{qt_te[2]:.3f}/{qt_te[3]:.3f}/{qt_te[4]:.3f}")

if 'y_win' in train.columns:
    corr = pd.Series(train['feat_soft_finish']).corr(pd.Series(train['y_win']).astype(float))
    print(f"[STAT] Corr(feat_soft_finish, y_win) on Train = {corr:.4f}")

# === 特徴量セットへ注入 =================================================
if 'feature_cols' in globals():
    if 'feat_soft_finish' not in feature_cols: feature_cols.append('feat_soft_finish')
elif 'FEATURES' in globals():
    if 'feat_soft_finish' not in FEATURES: FEATURES.append('feat_soft_finish')
elif 'FEATURE_COLS' in globals():
    if 'feat_soft_finish' not in FEATURE_COLS: FEATURE_COLS.append('feat_soft_finish')

# === メタ保存（today 用） ==============================================
try:
    ART_DIR_C = globals().get('ART_DIR_C', "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_c")
    os.makedirs(ART_DIR_C, exist_ok=True)

    # 1) Soft-Finish モデル保存
    SF_MODEL_PATH = os.path.join(ART_DIR_C, "JRA_stage_c_softfinish_model.pkl")
    final_sf.save_model(SF_MODEL_PATH)

    # 2) 既存メタを読み込み（なければ空dict）
    meta_path = globals().get('STAGE_C_META_PATH', os.path.join(ART_DIR_C, "JRA_stage_c_meta.json"))
    meta = {}
    if os.path.exists(meta_path):
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

    # 3) Soft-Finish で使用したカテゴリ集合（本体と完全一致）を準備
    #    - CAT_MAP_FINAL がある場合：Cell-4 で確定させた categories を使用（推奨）
    #    - ない場合：フォールバックで train の .cat.categories を使用
    if 'CAT_MAP_FINAL' in globals():
        cat_categories_sf = {
            c: [str(x) for x in CAT_MAP_FINAL[c]]
            for c in cat_used_sf
            if c in CAT_MAP_FINAL
        }
    else:
        cat_categories_sf = {}
        for c in cat_used_sf:
            if c in train.columns and hasattr(train[c].dtype, "categories"):
                # dtype.categories をそのまま保存（順序も保持）
                cat_categories_sf[c] = [str(x) for x in train[c].cat.categories.astype("string")]

    # 4) Soft-Finish メタを上書き（既存キーは保持・必要キーのみ更新）
    meta.setdefault("soft_finish", {})
    meta["soft_finish"].update({
        "model_path": SF_MODEL_PATH,
        "num_boost_round": int(num_boost_round_sf),
        "feature_cols": FEAT_COLS_SF,
        "categorical_cols": cat_used_sf,
        "cat_categories": cat_categories_sf,   # ← 本体と同一カテゴリ集合を保存
        # stage-today で同じ caps を使うため
        "winsor": {
            "q_low":  float(WINSOR_Q_LOW),
            "q_high": float(WINSOR_Q_HIGH),
            "lo":     float(lo),
            "hi":     float(hi),
        }
    })

    # 5) 書き出し
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    print(f"[INFO] Soft-Finish モデル/メタを保存しました: {SF_MODEL_PATH}")

except Exception as e:
    print(f"[WARN] Soft-Finishの最終学習/保存で警告: {e}")

print("→ 以降の LightGBM（二値・単勝確率）の学習は、追加済みの 'feat_soft_finish' を含む feature_cols でそのまま実行されます。")

# ────────────────────────────────────────────────
# Cell-4.7 : サンプル重み（低配当ゲートを含む最終重み）
#   - 目的: 低配当の勝利（例: 単勝 < 300円）の学習寄与を抑制/除外
#   - 出力: train["__sample_weight_stage_c__"] を最終確定
#           valid/test は 1.0（評価は素の確率指標で）
# 環境変数:
#   LOW_PAYOUT_MIN_YEN     ... しきい値（デフォルト=300）
#   LOW_PAYOUT_SOFT_WIDTH  ... ソフト遷移幅（0=ハードゲート）
# ────────────────────────────────────────────────
print("\n" + "="*50)
print("◎ Sample Weights build（低配当ゲート）")
print("="*50)

import numpy as np
import pandas as pd
import os

MIN_YEN   = int(os.environ.get("LOW_PAYOUT_MIN_YEN", "300"))   # 300円未満を抑制
SOFT_W    = int(os.environ.get("LOW_PAYOUT_SOFT_WIDTH", "0"))  # 0=ハード、それ以外=ソフト幅（例:150）

def _build_low_payout_gate_weight(
    df_tr: pd.DataFrame,
    payout_col: str = "win_payout",
    min_yen: int = 300,
    soft_width: int = 0
) -> np.ndarray:
    """勝者だけを対象に、低配当の学習寄与を抑えるゲート（0〜1）。
    - soft_width=0: ハード（min_yen未満は0、以上は1）
    - soft_width>0: min_yen→min_yen+soft_width の間で 0→1 に線形遷移
    """
    pay = pd.to_numeric(df_tr.get(payout_col, 0), errors="coerce").fillna(0).astype("float64").values
    gate = np.ones(len(df_tr), dtype="float64")
    if "y_win" not in df_tr.columns:
        raise RuntimeError("[FATAL] y_win が見つかりません。Cell-4 での目的変数作成を確認してください。")
    m_win = (df_tr["y_win"].values == 1)

    if soft_width <= 0:
        # ハード: 単勝 < min_yen の勝利は weight=0
        gate[m_win & (pay < float(min_yen))] = 0.0
    else:
        # ソフト: [min_yen, min_yen+soft_width] で 0→1 線形
        lo, hi = float(min_yen), float(min_yen + soft_width)
        width = max(hi - lo, 1e-9)
        g = (pay - lo) / width
        g = np.clip(g, 0.0, 1.0)
        gate[m_win] = g[m_win]

    return gate

# 既存の重みがあればそれに掛け算、なければゲートのみで初期化
W_gate = _build_low_payout_gate_weight(train, payout_col="win_payout", min_yen=MIN_YEN, soft_width=SOFT_W)

if "__sample_weight_stage_c__" in train.columns:
    base_w = pd.to_numeric(train["__sample_weight_stage_c__"], errors="coerce").fillna(1.0).astype("float64").values
    W_final = (base_w * W_gate).astype("float64")
else:
    W_final = W_gate.astype("float64")

# 最終重みを列として保存（学習で使用）
train["__sample_weight_stage_c__"] = W_final.astype("float32")

# 参考: valid/test は評価だけなので 1.0 にしておく（既に列があるなら欠損を1.0で埋め）
for _df in (valid, test):
    if "__sample_weight_stage_c__" not in _df.columns:
        _df["__sample_weight_stage_c__"] = 1.0
    else:
        _df["__sample_weight_stage_c__"] = pd.to_numeric(_df["__sample_weight_stage_c__"], errors="coerce").fillna(1.0).astype("float32")

# サマリ出力
def _wstat(name, w):
    w = np.asarray(w, dtype="float64")
    print(f" - {name:<12} mean={w.mean():.4f}  min={w.min():.3f}  p50={np.quantile(w,0.5):.3f}  p90={np.quantile(w,0.9):.3f}  max={w.max():.1f}")

print("[Weights Summary]")
_wstat("gate(lowP)", W_gate)
_wstat("FINAL(train)", train["__sample_weight_stage_c__"].values)
print("（valid/test は 1.0 固定）")


# Title: FEATURES 統一の安全エイリアス
# Description: どちらか片方の変数名しか無い環境でもズレないようにする（最終的に FEATURES を一本化）
if 'FEATURES' not in globals() and 'feature_cols' in globals():
    FEATURES = feature_cols[:]  # ← コピーで受ける
elif 'FEATURES' in globals() and 'feature_cols' in globals():
    # 片方にしか無い列が混じっていないか簡易チェック（任意）
    pass

# Title: 市場“生値”の直接使用を停止（A / A+）
# Description: trifecta_support_rate, tsr_prob と popularity/odds 文字列を含む列を学習から落とす
_before = len(FEATURES)
DROP_KEYS = ("popularity", "odds")
FEATURES = [c for c in FEATURES
            if c != "trifecta_support_rate"   # A
            and c != "tsr_prob"               # A+（相対: z_tsr_prob 等は残す）
            and not any(k in c for k in DROP_KEYS)]  # A+（市場痕跡の代理）
print(f"[PATCH-A/A+] features: {_before} -> {len(FEATURES)} "
      f"(drop trifecta_support_rate, tsr_prob, *popularity*, *odds*)")

# Title: CAND 閾値系の一時外し（C）
# Description: exclude_margin_* は “安全側バイアス” が強く ROI を押し下げやすいので検証目的で外す
_sus = [c for c in FEATURES if c.startswith("exclude_margin_")]
if _sus:
    FEATURES = [c for c in FEATURES if c not in _sus]
    print(f"[PATCH-C] drop suspects: {_sus}")

# （念のため）存在確認
_missing = [c for c in FEATURES if c not in train.columns]
assert not _missing, f"FEATURES中に存在しない列があります: {_missing[:5]}"

# タイトル: 市場生値の直接使用を停止（FEATURESへ確実適用）
# 説明: 'trifecta_support_rate' / 'tsr_prob' / '*popularity*' / '*odds*' を FEATURES から除外

_before = len(FEATURES)
DROP_KEYS = ("popularity", "odds")

FEATURES = [
    c for c in FEATURES
    if (c != "trifecta_support_rate")
    and (c != "tsr_prob")
    and not any(k in c for k in DROP_KEYS)
]
print(f"[PATCH-A/A+] FEATURES drop: {_before} -> {len(FEATURES)} (removed 'trifecta_support_rate','tsr_prob','*popularity*','*odds*')")

# タイトル: CAND閾値列の一時除外（検証）
# 説明: exclude_margin_* をいったん外して AUC/ROI 影響を切り分け
_sus = [c for c in FEATURES if c.startswith("exclude_margin_")]
if _sus:
    FEATURES = [c for c in FEATURES if c not in _sus]
    print(f"[PATCH-C] drop suspects from FEATURES: {_sus}")



# ------------------------
# Cell-5: LightGBM 学習（単勝的中率＝binary, 各種改善を反映）
#   ② Lambdarank並走（切替フラグなしで併走学習）
#   ④ 配当重み（勝者のみ上乗せ・学習時のみ）
#   ⑤ モノトニック制約（主要確率系の単調性を担保）
#   ⑥ 時系列重み（最近データを重視）
#   ⑦ 小規模アンサンブル（N本の平均；既定3）
# ------------------------
print("\n--- Cell-5: LightGBM学習（単勝的中率＝binary / 改善一式） ---")

import os, numpy as np, pandas as pd, lightgbm as lgb
from copy import deepcopy
from pathlib import Path


# ==============================================================================
# === アイデア1-A：Soft-Finish × SP の相互作用特徴量（Cell-5 冒頭） ===
#   挿入位置：Cell-5の最初（print直後など）、FEATURES抽出の前
# ==============================================================================
print("\n[INFO] 相互作用特徴量(SF × zSP) を作成します...")
_sf_interact = []

for _df in (train, valid, test):
    if "feat_soft_finish" in _df.columns and "z_pred_time_index_sp" in _df.columns:
        _df["interact_SF_x_SPz"] = (
            _df["feat_soft_finish"].fillna(0.0).astype("float32")
            * _df["z_pred_time_index_sp"].fillna(0.0).astype("float32")
        ).astype("float32")
        _sf_interact.append("interact_SF_x_SPz")

print(f"   - 新規作成: {sorted(set(_sf_interact))}")

# FEATURES は Cell-4 で確定済みなので、必要なら念のため追加
if "FEATURES" in globals():
    for _n in sorted(set(_sf_interact)):
        if _n not in FEATURES:
            FEATURES.append(_n)
            print(f"   - '{_n}' を FEATURES に追加しました。")
# ==============================================================================



# 万一 'feat_soft_finish' が FEATURES に未追加なら注入（存在チェック付き）
if "feat_soft_finish" in train.columns and "feat_soft_finish" not in FEATURES:
    FEATURES.append("feat_soft_finish")

# ---------- Helper ----------
def _ensure_category(df, cols):
    for c in cols:
        if c in df.columns and df[c].dtype.name != "category":
            df[c] = df[c].astype("category")

def _monotone_constraints_for(features):
    """
    主要“勝率↑で良い”系は +1、明確に逆相関のものは -1、それ以外は 0。
    ※ カテゴリ列にも要素数合わせで 0 を付与（LGBM側で無視される）
    """
    pos_keys = {
        "pred_prob_stage_a", "pred_ev_stage_b_any", "feat_soft_finish",
        "tsr_prob", "keep_prob_in3", "pred_time_index_sp"
    }
    neg_keys = {"pred_odds"}  # あれば明確に逆単調（オッズ高い=勝率低い）
    cons = []
    for f in features:
        if f in pos_keys:
            cons.append(1)
        elif f in neg_keys:
            cons.append(-1)
        else:
            cons.append(0)
    return cons

def _compute_recency_weight(df, alpha=0.40):
    """
    alpha in [0,1]: 0→全期間=1, 1→最古~最新で線形に 0→1 へ
    実際は (1 - alpha) + alpha * norm で最古~最新が [1-alpha, 1] に。
    """
    d = pd.to_datetime(df["date"]).view("int64")
    if d.max() == d.min():
        return np.ones(len(df), dtype="float32")
    norm = (d - d.min()) / (d.max() - d.min())
    return ((1.0 - alpha) + alpha * norm).astype("float32")

def _compute_payout_weight(df, y, bet_unit=100, alpha=0.08, cap_mult=25.0):
    """
    勝者のみ上乗せ: 1 + alpha * (min(unit_return, cap) - 1)
    loserは1.0。学習時のみ使用（validは1.0で固定）。
    """
    win = pd.to_numeric(df.get("win_payout", 0), errors="coerce").fillna(0.0).values
    unit_return = (win / float(bet_unit)).clip(0, cap_mult)
    w = np.ones(len(df), dtype="float32")
    w[y.astype(bool)] = 1.0 + alpha * (unit_return[y.astype(bool)] - 1.0)
    return w.astype("float32")

def _group_order_and_sizes(df, keys=("date","race_code")):
    """
    Lambdarank用に、同一レースを連続させる並びに整列し、
    連続区間サイズ配列（group）を返す。
    """
    gkey = df[list(keys)].apply(tuple, axis=1).to_numpy()
    # 連続化のため、キーで安定ソート
    order = np.argsort(gkey, kind="mergesort")
    g_sorted = gkey[order]
    # 連続区間サイズ
    sizes = []
    cnt = 1
    for i in range(1, len(g_sorted)):
        if g_sorted[i] != g_sorted[i-1]:
            sizes.append(cnt)
            cnt = 1
        else:
            cnt += 1
    if len(g_sorted) > 0:
        sizes.append(cnt)
    return order, np.array(sizes, dtype=np.int32)

class EnsembleLGBM:
    """小規模アンサンブル（単純平均）"""
    def __init__(self, boosters):
        self.boosters = boosters
        # 代表の best_iteration（中央値）を持たせるだけ（predictでは未使用でもOK）
        its = [int(b.best_iteration or b.current_iteration()) for b in boosters]
        self.best_iteration = int(np.median(its)) if len(its) else None

    def predict(self, X, num_iteration=None, raw_score=False):
        preds = []
        for b in self.boosters:
            preds.append(b.predict(X, num_iteration=num_iteration, raw_score=raw_score))
        return np.mean(np.vstack(preds), axis=0)

# ---------- データ準備 ----------
X_tr = train[FEATURES].copy(); y_tr = train["y_win"].astype("int8").values
X_va = valid[FEATURES].copy(); y_va = valid["y_win"].astype("int8").values

# カテゴリ列（存在確認しつつ category 型を担保）
cat_cols_exist = [c for c in (cat_cols_final if 'cat_cols_final' in globals() else []) if c in X_tr.columns]
_ensure_category(X_tr, cat_cols_exist)
_ensure_category(X_va, cat_cols_exist)

pos_rate = float(y_tr.mean()); print(f"   - pos_rate={pos_rate:.4f} ({pos_rate*100:.2f}%)")

# ---------- 重み（配当 & 時系列；学習時のみ） ----------
SEED = int(globals().get("SEED", 42))
BET_UNIT = int(globals().get("BET_UNIT", 100))
PAYOUT_ALPHA   = float(os.environ.get("PAYOUT_ALPHA", "0.08"))   # 推奨 0.05〜0.10
PAYOUT_CAPMULT = float(os.environ.get("PAYOUT_CAPMULT", "25.0")) # 単位払戻の上限（例: 25倍=2,500円/100円）
RECENCY_ALPHA  = float(os.environ.get("RECENCY_ALPHA", "0.40"))  # 0.2〜0.6 目安

w_payout  = _compute_payout_weight(train, y_tr, bet_unit=BET_UNIT, alpha=PAYOUT_ALPHA, cap_mult=PAYOUT_CAPMULT)
w_recency = _compute_recency_weight(train, alpha=RECENCY_ALPHA)

# ▼ ここを追加：Cell-4.7 で作った“低配当ゲート”を掛け合わせる
if "__sample_weight_stage_c__" in train.columns:
    w_gate = pd.to_numeric(train["__sample_weight_stage_c__"], errors="coerce").fillna(1.0).astype("float32").values
else:
    # 念のためのフォールバック（列が無ければゲート=1）
    w_gate = np.ones(len(train), dtype="float32")

# 3つの重みを合成
train_weight = (w_payout * w_recency * w_gate).astype("float32")

# （任意）平均1に正規化して学習率の実効スケールを安定化
if os.environ.get("RENORM_WEIGHTS", "1") == "1":
    m = float(train_weight.mean())
    if m > 0:
        train_weight = (train_weight / m).astype("float32")

# 検証は等重み（評価の解釈を保つ）
valid_weight = np.ones(len(X_va), dtype="float32")

print(f"   - payout_alpha={PAYOUT_ALPHA}, cap_mult={PAYOUT_CAPMULT}, recency_alpha={RECENCY_ALPHA}")
print(f"   - weight stats (payout): mean={w_payout.mean():.3f} p90={np.percentile(w_payout,90):.3f} max={w_payout.max():.3f}")
print(f"   - weight stats (recency): mean={w_recency.mean():.3f} p90={np.percentile(w_recency,90):.3f} max={w_recency.max():.3f}")
print(f"   - weight stats (gate): mean={w_gate.mean():.3f} p90={np.percentile(w_gate,90):.3f} max={w_gate.max():.1f}")
print(f"   - weight stats (final): mean={train_weight.mean():.3f} p90={np.percentile(train_weight,90):.3f} max={train_weight.max():.3f}")

# ---------- パラメータ共通部 ----------
base_params = {
    "boosting_type": "gbdt",
    "seed": SEED,
    "first_metric_only": True,
    "force_col_wise": True,   # メモリ安定
    "verbosity": -1,
    "num_threads": -1,
    "max_bin": 255,
    # "deterministic": True,  # 必要に応じて
}

# Optuna最適化済み（ユーザ提供）
optuna_best = {
    "learning_rate": 0.05918612051715358,
    "num_leaves": 42,
    "min_data_in_leaf": 179,
    "feature_fraction": 0.8789678670695426,
    "bagging_fraction": 0.9262282927649069,
    "bagging_freq": 7,
    "lambda_l1": 3.263184015661723,
    "lambda_l2": 0.0034617269200193494,
    "min_gain_to_split": 0.016927260477654776,
    "feature_fraction_bynode": 0.6790387697880631,
}

# モノトニック制約
mono = _monotone_constraints_for(FEATURES)

# （分類）最終パラメータ
bin_params_base = {
    **base_params,
    **optuna_best,
    "objective": "binary",
    "metric": "binary_logloss",
    "monotone_constraints": mono,
}

# タイトル: 整列＆陽性率ガード
# 説明: X_tr, y_tr, train_weight の行順一致／Validのpos_rateを明示
assert np.array_equal(X_tr.index.values, train.index.values), \
    "[ERR] X_tr と train のindex不一致（整列処理を見直してください）"
assert len(X_tr) == len(y_tr) == len(train_weight), \
    "[ERR] 学習配列の長さ不一致（features/labels/weights）"

print(f"[SANITY] Valid pos_rate = {y_va.mean():.4f} (n={len(y_va)})")


# Dataset（学習は重み付き／検証は等重み）
lgb_tr = lgb.Dataset(X_tr, label=y_tr, weight=train_weight,
                     categorical_feature=cat_cols_exist, free_raw_data=False, feature_name=FEATURES)
lgb_va = lgb.Dataset(X_va, label=y_va, #weight=valid_weight,
                     categorical_feature=cat_cols_exist, reference=lgb_tr, free_raw_data=False, feature_name=FEATURES)

callbacks = [
    lgb.early_stopping(stopping_rounds=100, verbose=True),
    lgb.log_evaluation(period=100)
]

print("[INFO] 使用パラメータ（要約｜binary）:")
_show = {k: bin_params_base[k] for k in [
    "learning_rate","num_leaves","min_data_in_leaf","feature_fraction","feature_fraction_bynode",
    "bagging_fraction","bagging_freq","lambda_l1","lambda_l2","min_gain_to_split"
] if k in bin_params_base}
print(_show)
print(f"[INFO] モノトニック制約（先頭20件預り表示）: {mono[:20]} ... len={len(mono)}")

# ---------- ⑦ 小規模アンサンブル（N本） ----------
ENSEMBLE_N = int(os.environ.get("STAGEC_ENSEMBLE_N", "2"))
ENSEMBLE_N = max(1, min(ENSEMBLE_N, 5))

bin_boosters = []
best_iters = []
for i in range(ENSEMBLE_N):
    seed_i = SEED + 11*i + 7  # 簡易にずらす
    params_i = deepcopy(bin_params_base)
    params_i.update({
        "seed": seed_i,
        "feature_fraction_seed": seed_i,
        "bagging_seed": seed_i,
        "data_random_seed": seed_i,
    })
    print(f"\n[Ensemble] Training binary model #{i+1}/{ENSEMBLE_N} (seed={seed_i})")
    booster = lgb.train(
        params=params_i,
        train_set=lgb_tr,
        valid_sets=[lgb_va, lgb_tr],
        valid_names=["valid","train"],
        num_boost_round=2000,
        callbacks=callbacks,
    )
    bin_boosters.append(booster)
    best_iters.append(int(booster.best_iteration or booster.current_iteration()))

print("[GUARD] early_stopping monitor = VALID (valid_sets[0])")
print(f"\n[INFO] Ensemble binary complete: best_iter (median/mean) = {np.median(best_iters):.0f} / {np.mean(best_iters):.1f}")
model = EnsembleLGBM(bin_boosters)  # ← 以降の Cell-X はこの model.predict(...) を使い平均予測

# 代表モデルの保存（#1 を保存／全体は任意で複数保存）
STAGE_C_MODEL_PATH = globals().get("STAGE_C_MODEL_PATH",
                                   os.path.join(ART_DIR_C, "JRA_stage_c_model.txt"))
Path(os.path.dirname(STAGE_C_MODEL_PATH)).mkdir(parents=True, exist_ok=True)
bin_boosters[0].save_model(STAGE_C_MODEL_PATH)
print(f"   ✅ モデル保存（代表#1）: {STAGE_C_MODEL_PATH}")

# 追加保存（任意）：他メンバーをサフィックス付きで保存
try:
    for i, bst in enumerate(bin_boosters[1:], start=2):
        p = STAGE_C_MODEL_PATH.replace(".txt", f"_m{i}.txt").replace(".pkl", f"_m{i}.txt")
        bst.save_model(p)
    print(f"   - 追加メンバーも保存（_m2..）")
except Exception as e:
    print(f"   [WARN] 追加メンバー保存に失敗: {e}")

# ---------- ② Lambdarank 並走（切替フラグなし） ----------
# 学習・検証をレース単位で連続化した順序に並べ直し、group を渡す
print("\n[Rank] Lambdarank 併走学習を開始します（ndcg@1,3,5）")
order_tr, group_tr = _group_order_and_sizes(train)
order_va, group_va = _group_order_and_sizes(valid)

X_tr_r = X_tr.iloc[order_tr]
y_tr_r = y_tr[order_tr]
X_va_r = X_va.iloc[order_va]
y_va_r = y_va[order_va]

lgb_tr_rank = lgb.Dataset(
    X_tr_r, label=y_tr_r, group=group_tr,
    categorical_feature=cat_cols_exist, free_raw_data=False, feature_name=FEATURES
)
lgb_va_rank = lgb.Dataset(
    X_va_r, label=y_va_r, group=group_va,
    categorical_feature=cat_cols_exist, reference=lgb_tr_rank, free_raw_data=False, feature_name=FEATURES
)

rank_params = {
    **base_params,
    **optuna_best,
    "objective": "lambdarank",
    "metric": "ndcg",
    "eval_at": [1,3,5],
    "seed": SEED + 777,
    "feature_fraction_seed": SEED + 777,
    "bagging_seed": SEED + 777,
    "data_random_seed": SEED + 777,
    # モノトニック制約は ranking でも有効（不要なら外してOK）
    "monotone_constraints": mono,
}

callbacks_rank = [
    lgb.early_stopping(stopping_rounds=100, verbose=True),
    lgb.log_evaluation(period=100),
]

print("[INFO] 使用パラメータ（要約｜lambdarank）:")
_show_r = {k: rank_params[k] for k in [
    "learning_rate","num_leaves","min_data_in_leaf","feature_fraction","feature_fraction_bynode",
    "bagging_fraction","bagging_freq","lambda_l1","lambda_l2","min_gain_to_split"
] if k in rank_params}
print(_show_r)

rank_model = lgb.train(
    params=rank_params,
    train_set=lgb_tr_rank,
    valid_sets=[lgb_tr_rank, lgb_va_rank],
    valid_names=["train_rank","valid_rank"],
    num_boost_round=2000,
    callbacks=callbacks_rank,
)
rank_best_iter = int(rank_model.best_iteration or rank_model.current_iteration())
print(f"   - rank_best_iter = {rank_best_iter}")

# 参考: 検証における簡易 Top1 指標（lambdarank単体の相性確認）
try:
    # そのまま logit 相当（raw_score=True）でレース内Top1の正解率
    from collections import defaultdict
    valid_rank_score = pd.Series(rank_model.predict(X_va, num_iteration=rank_best_iter, raw_score=True))
    key_va = valid[["date","race_code"]].apply(tuple, axis=1).to_numpy()
    df_tmp = pd.DataFrame({"k": key_va, "y": y_va, "s": valid_rank_score.values})
    top1 = df_tmp.sort_values("s", ascending=False).groupby("k", sort=False).head(1)
    top1_acc = float(top1["y"].mean()) if len(top1) else float("nan")
    print(f"   [Rank] Valid Top1 accuracy (参考): {top1_acc*100:.2f}%")
except Exception as e:
    print(f"   [WARN] Rankの参考評価で例外: {e}")

# 代表 booster の best_iteration（中央値）を表示（Cell-Xのログ整合）
best_iter = getattr(model, "best_iteration", None)
print(f"\n   - best_iter (ensemble median) = {best_iter}  （参考：個別={best_iters[:min(5,len(best_iters))]}）")

# グローバルへ露出（Cell-X が拾えるように）
globals()["model"] = model               # ← 以降の Cell-X はこれを使って予測（平均）
globals()["rank_model"] = rank_model     # ← 併走モデル（分析・将来の活用用）

# 代表(#1)と追加メンバーは上で保存済み
print("   ✅ モデル保存: 代表(#1)＋メンバー（_m2..）を保存済み")
# 互換用パス（代表#1）: STAGE_C_MODEL_PATH をそのまま stage-today が使えます

# --- Compat: 後段セルが探す単一Boosterをエクスポート（Ensemble対応版） ---
try:
    booster_alias = None

    # 1) すでに Booster 単体ならそのまま
    if isinstance(model, lgb.Booster):
        booster_alias = model

    # 2) EnsembleLGBM（今回の実装）：.boosters の先頭を採用
    if booster_alias is None and hasattr(model, "boosters"):
        bs = getattr(model, "boosters", None)
        if isinstance(bs, (list, tuple)) and len(bs) > 0 and isinstance(bs[0], lgb.Booster):
            booster_alias = bs[0]

    # 3) 念のため学習時の変数も見る
    if booster_alias is None and "bin_boosters" in globals():
        bs = globals()["bin_boosters"]
        if isinstance(bs, (list, tuple)) and len(bs) > 0 and isinstance(bs[0], lgb.Booster):
            booster_alias = bs[0]

    # 4) 旧実装の互換
    if booster_alias is None and hasattr(model, "members"):
        ms = getattr(model, "members", None)
        if isinstance(ms, (list, tuple)) and len(ms) > 0 and isinstance(ms[0], lgb.Booster):
            booster_alias = ms[0]
    if booster_alias is None and hasattr(model, "models"):
        ms = getattr(model, "models", None)
        if isinstance(ms, (list, tuple)) and len(ms) > 0 and isinstance(ms[0], lgb.Booster):
            booster_alias = ms[0]

    if booster_alias is not None:
        # 後段セルが探す名前を“全部”用意しておく
        gbm = booster_alias
        gbm_final = booster_alias
        bst = booster_alias

        # best_iter（代表値）も置いておく
        try:
            _its = []
            if isinstance(model, lgb.Booster):
                _its = [getattr(model, "best_iteration", 0) or 0]
            elif hasattr(model, "boosters"):
                _its = [getattr(m, "best_iteration", 0) or 0 for m in model.boosters]
            elif hasattr(model, "members"):
                _its = [getattr(m, "best_iteration", 0) or 0 for m in model.members]
            elif hasattr(model, "models"):
                _its = [getattr(m, "best_iteration", 0) or 0 for m in model.models]
            best_iter = int(np.median([i for i in _its if i])) if _its else best_iter
        except Exception:
            pass

        print("[Compat] Exported Booster alias as 'gbm' / 'gbm_final' / 'bst'（best_iter≈%s）" % str(best_iter))
    else:
        print("[Compat] Booster alias not exported（unknown model container）")

except Exception as e:
    print(f"[Compat][WARN] Booster aliasing failed: {e}")



# ────────────────────────────────────────────────
# Cell-X : OOF（raw_logit）+ レース内 softmax + 温度スケーリング + 確率評価
# ────────────────────────────────────────────────
print("\n" + "="*50)
print("◎ OOF予測・softmax較正・温度スケーリング（Stage-C 単勝）")
print("="*50)

import os, json, gc
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import KFold, TimeSeriesSplit
from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss
from scipy.optimize import minimize


# タイトル: 動的T(g)の縮退を防止（弱いL2＋Tレンジ微修正）
# 説明: Tが全レース0.90に張り付く縮退を、L2正則化＋Tの上下限を狭めて回避します。
#       既存関数には触らず、このセル内だけで完結（最小変更）。
# ▼ 既存の T_MIN/T_MAX があっても上書きしません（別名で運用）
# タイトル: 動的T(g)の弱正則化＆レンジ微調整
# 説明: 過信も過度平滑も避けるため、T_min=0.90, T_max=1.20 に。
PATCH_T_MIN = 0.90    # 以前の 0.95 より下げ、HHI高のレースで少しだけ鋭くできる余地を残す
PATCH_T_MAX = 1.20
PATCH_L2    = 5e-4    # ごく弱く

def _learn_dynamic_T_from_oof_patch(raw_logit_oof, y_train_bool, df_train):
    import numpy as np
    from scipy.optimize import minimize
    gid, hhi_g, logn_g = _group_metrics_for_T(df_train)

    def _T_from(theta):
        a0,a1,a2 = theta
        Tg = np.exp(a0 + a1*hhi_g + a2*logn_g)
        # HHIが高い（本命集中）時はわずかに下振れを許容（最大0.05だけ下）
        floor = PATCH_T_MIN - 0.05*(hhi_g > np.quantile(hhi_g, 0.8))
        return np.clip(Tg, floor, PATCH_T_MAX)

    def _nll(theta):
        Tg = _T_from(theta)
        p  = _softmax_by_group_dynamicT(raw_logit_oof, gid, Tg)
        eps=1e-15
        return -(np.log(np.clip(p[y_train_bool], eps, 1.0)).mean()) + PATCH_L2*np.sum(theta*theta)

    theta0 = np.array([0.0, -0.3, 0.2])
    res = minimize(_nll, x0=theta0, method="L-BFGS-B")
    theta = res.x
    Tg = _T_from(theta)
    print(f"[PATCH-B] dynamic T(g): min={Tg.min():.3f} p10={np.percentile(Tg,10):.3f} "
          f"med={np.median(Tg):.3f} p90={np.percentile(Tg,90):.3f} max={Tg.max():.3f}")
    return theta  # (a0,a1,a2)



# ---- 既存オブジェクトを取得（環境差を吸収） -----------------------
def _get_feature_cols():
    if 'feature_cols' in globals(): return feature_cols
    if 'FEATURES' in globals():     return FEATURES
    raise RuntimeError("[FATAL] feature_cols / FEATURES が見つかりません。")

def _get_cat_cols():
    for name in ('cat_cols_final','cat_cols','categorical_cols'):
        if name in globals():
            return globals()[name]
    return []  # 無ければ空でOK

def _get_model_and_best_iter():
    for name in ('model','gbm_final','gbm'):
        if name in globals():
            m = globals()[name]
            bi = getattr(m, 'best_iteration', None)
            return m, (int(bi) if bi is not None else None)
    raise RuntimeError("[FATAL] 学習済み LightGBM モデルが見つかりません（model / gbm_final / gbm）。")

FEATURE_COLS = _get_feature_cols()
CAT_COLS     = _get_cat_cols()
TARGET       = 'y_win'
BET_UNIT     = BET_UNIT if 'BET_UNIT' in globals() else 100

# ---- レース内 softmax（数値安定化） -------------------------------
def _group_ids(df, keys=("date","race_code")):
    # groupキーを高速に整数ID化
    gser = df[list(keys)].agg(tuple, axis=1)
    _, gid = np.unique(gser.values, return_inverse=True)
    return gid  # 0..G-1

def _softmax_by_group(raw_logit, gid, T=1.0):
    # raw_logit: 1次元 np.array（logit=raw_score）
    # gid: 0..G-1 の整数グループID
    eps = 1e-15
    z = raw_logit / max(T, eps)
    gmax = np.full(gid.max()+1, -np.inf, dtype=np.float64)
    np.maximum.at(gmax, gid, z)
    zmax = gmax[gid]
    ex = np.exp(z - zmax)
    gsum = np.zeros_like(gmax)
    np.add.at(gsum, gid, ex)
    p = ex / np.clip(gsum[gid], eps, None)
    return np.clip(p, eps, 1 - eps).astype('float64')

# ========= ここから「動的温度 T(g)」の追加 =========
# TSRをレース内で確率スケール（合計=1）に正規化
def _tsr_prob_norm(df, tsr_col="trifecta_support_rate"):
    # レースキーを 0..G-1 の整数IDへ
    keys = df[["date","race_code"]].apply(tuple, axis=1).values
    _, gid = np.unique(keys, return_inverse=True)  # contiguous group ids

    # TSRをfloatに
    s = pd.to_numeric(df[tsr_col], errors="coerce").fillna(0.0).astype("float64").values

    # group-wise の合計を np.add.at で作る
    G = int(gid.max()) + 1
    denom = np.zeros(G, dtype=np.float64)
    np.add.at(denom, gid, s)

    # 各行の分母を引き当てて正規化（合計=1）
    denom_safe = np.clip(denom[gid], 1e-12, None)
    return (s / denom_safe), gid  # tsr_prob, gid


def _group_metrics_for_T(df):
    # レース単位の HHI と log(頭数) を作る
    tsr_p, gid = _tsr_prob_norm(df)
    G = int(gid.max()) + 1

    p2_sum = np.zeros(G, dtype=np.float64)
    cnt    = np.zeros(G, dtype=np.int32)
    np.add.at(p2_sum, gid, tsr_p * tsr_p)  # HHI = Σ p_i^2
    np.add.at(cnt,    gid, 1)

    hhi_g  = p2_sum
    logn_g = np.log(np.maximum(cnt, 1))
    return gid, hhi_g, logn_g

# パラメトリックT: T(g) = T_min + (T_max-T_min) * sigmoid(a0 + a1*HHI + a2*logN)
T_MIN, T_MAX = 0.90, 1.45  # 安定レンジ（必要に応じて調整可）

def _T_from_params(params, hhi_g, logn_g, tmin=T_MIN, tmax=T_MAX):
    a0, a1, a2 = params
    s = 1.0 / (1.0 + np.exp(-(a0 + a1*hhi_g + a2*logn_g)))
    return tmin + (tmax - tmin) * s  # shape: (G,)

def _softmax_by_group_dynamicT(raw_logit, gid, T_group):
    """グループごと温度が異なるsoftmax"""
    eps = 1e-15
    z = raw_logit / np.clip(T_group[gid], eps, None)

    G = int(gid.max()) + 1
    gmax = np.full(G, -np.inf, dtype=np.float64)
    np.maximum.at(gmax, gid, z)
    zmax = gmax[gid]

    ex = np.exp(z - zmax)
    gsum = np.zeros(G, dtype=np.float64)
    np.add.at(gsum, gid, ex)

    p = ex / np.clip(gsum[gid], eps, None)
    return np.clip(p, eps, 1 - eps).astype("float64")

# タイトル: 動的T(g)学習（重み付きNLL対応・完全差し替え版）
# 説明:
#   OOFの logit と同一のサンプル重みで NLL を最小化し、較正の分布ズレを解消します。
#   既存の振る舞いは weight=None のときと等価（後方互換）。
#   L2正則化はデフォルトOFF（必要時のみ l2>0 を指定）。
def _learn_dynamic_T_from_oof(raw_logit_oof, y, df_tr, weight=None, l2=0.0, normalize_weight=True):
    """
    Args:
        raw_logit_oof : np.ndarray, shape (N,)
            OOF上の raw logit（レース内softmax前のスコア。Nは学習サンプル数）
        y : array-like, shape (N,)
            目的変数（1=勝ち馬, 0=その他）。内部では bool に変換します
        df_tr : pandas.DataFrame, shape (N, *)
            学習サンプル行に対応する行。race単位のID/統計を作るために使います
        weight : array-like or None, shape (N,), optional
            サンプル重み（Cell-4.7で作成した "__sample_weight_stage_c__" 等）
            None の場合は従来どおり等重み
        l2 : float, optional
            θ=(a0,a1,a2) に対する L2 正則化係数。デフォルト 0.0
        normalize_weight : bool, optional
            重みの平均を1に正規化してスケールを揃える（最適化の安定化用）

    Returns:
        np.ndarray(dtype=float, shape=(3,)) : 学習した (a0, a1, a2)
    """
    import numpy as np
    from scipy.optimize import minimize

    # --- 前処理（型/NaN クリーニング）---
    raw_logit_oof = np.asarray(raw_logit_oof, dtype=np.float64)
    y = np.asarray(y).astype(bool)
    if raw_logit_oof.shape[0] != y.shape[0] or raw_logit_oof.shape[0] != len(df_tr):
        raise ValueError("[FATAL] _learn_dynamic_T_from_oof: 長さが一致しません。raw_logit_oof, y, df_tr を確認してください。")

    m = np.isfinite(raw_logit_oof)
    if weight is not None:
        w = np.asarray(weight, dtype=np.float64)
        if w.shape[0] != raw_logit_oof.shape[0]:
            raise ValueError("[FATAL] weight の長さが合いません。")
        m &= np.isfinite(w)
    else:
        w = None

    # 有効サンプルへ圧縮（マスク順序を厳密に保持）
    z   = raw_logit_oof[m]
    yy  = y[m]
    dfm = df_tr.loc[m].copy()
    if w is not None:
        ww = w[m]
        if normalize_weight:
            ww = ww / (ww.mean() + 1e-12)  # 平均1にスケール
    else:
        ww = None

    # レース単位のグループ情報を構築（既存のヘルパーを利用）
    gid, hhi_g, logn_g = _group_metrics_for_T(dfm)

    # --- 目的関数（重み付きNLL + L2）---
    #  p は「各行の“正解クラス（勝ち馬=1）”の確率」に対応するベクトルを想定
    def _nll(theta):
        T_group = _T_from_params(theta, hhi_g, logn_g)
        p = _softmax_by_group_dynamicT(z, gid, T_group)  # shape=(len(z),)
        # 勝ち馬行だけを抽出（勝者のlog pの合計を最大化 ≒ NLL最小化）
        pw = p[yy]
        if pw.size == 0:
            return 1e9  # まれな異常系
        eps = 1e-15
        log_pw = np.log(np.clip(pw, eps, 1.0))
        if ww is None:
            loss = -np.mean(log_pw)
        else:
            # 重みは勝者行に対応するものだけを抽出
            ww_pos = ww[yy]
            loss = -np.average(log_pw, weights=ww_pos)
        if l2 > 0.0:
            loss += float(l2) * float(np.dot(theta, theta))
        return loss

    # --- 最適化 ---
    # 初期値の意味：市場集中（HHI↑）でシャープ（a1<0）、頭数多いとソフト（a2>0）
    theta0 = np.array([0.0, -2.0, 0.8], dtype=np.float64)
    res = minimize(_nll, x0=theta0, method="L-BFGS-B")

    # ログ出力（デバッグ用）
    try:
        a0, a1, a2 = [float(v) for v in res.x]
        print(f"[INFO] Learn Dynamic T(g) (weighted={ww is not None}) "
              f"a0={a0:.4f}, a1={a1:.4f}, a2={a2:.4f}, nll={float(res.fun):.6f}, "
              f"iters={getattr(res, 'nit', None)}")
    except Exception:
        pass

    return np.asarray(res.x, dtype=float)  # (a0, a1, a2)


def _probs_from_logit_dynamicT(df, raw_logit, params):
    gid, hhi_g, logn_g = _group_metrics_for_T(df)
    T_group = _T_from_params(params, hhi_g, logn_g)
    return _softmax_by_group_dynamicT(raw_logit.astype("float64"), gid, T_group)
# ========= 「動的温度 T(g)」追加ここまで =========

# タイトル: OOF（raw_logit）作成ブロック（インデント修正済み）
# 説明: OOF用のLightGBMを本学習条件に揃え、重みは位置参照で安全に抽出する

# ---- OOF（raw_logit）作成 ------------------------------------------
N_SPLITS = int(os.environ.get("STAGEC_OOF_K", 5))
USE_TSS  = os.environ.get("STAGEC_OOF_TSS", "1") == "1"  # ← デフォルトを TimeSeriesSplit（リーク対策）
print(f"[INFO] OOFを作成します: n_splits={N_SPLITS} / splitter={'TimeSeriesSplit' if USE_TSS else 'KFold'}")
splitter = TimeSeriesSplit(n_splits=N_SPLITS) if USE_TSS else KFold(n_splits=N_SPLITS, shuffle=False)

oof_logit = np.zeros(len(train), dtype='float64')  # raw_score（logit）を格納
best_iters = []

# 1) 学習パラメータのベースを拾う（無ければデフォルト）
def _pick_lgb_params():
    for k in ("lgb_params", "params", "PARAMS_BASE"):
        if k in globals() and isinstance(globals()[k], dict):
            return globals()[k]
    return {
        "objective": "binary",
        "metric": "binary_logloss",
        "learning_rate": 0.05,
        "num_leaves": 64,
        "seed": 42,
        "verbosity": -1,
    }

# 2) 浅いコピー → OOF向けの上書き（分散化パッチ）
# --- 置換後（最小差分）：本学習の bin_params_base / mono を流用 ---
params_oof = {**bin_params_base}     # ← Cell-5 で定義済み
params_oof.update({
    "objective": "binary",
    "metric": "binary_logloss",
    "first_metric_only": True,
    "monotone_constraints": mono,    # ← 同じ制約を適用
})
for k in ("feature_fraction_seed", "bagging_seed", "data_random_seed", "seed"):
    params_oof.setdefault(k, 42)

_dataset_params = {k: params_oof[k] for k in [
    "max_bin", "min_data_in_bin", "two_round", "data_random_seed",
    "force_col_wise", "histogram_pool_size", "bin_construct_sample_cnt",
    "max_cat_to_onehot", "max_cat_threshold", "min_data_per_group",
    "cat_l2", "cat_smooth"
] if k in params_oof}

print("[INFO] OOF LightGBM パラメータ（要約）:", {
    k: params_oof.get(k) for k in
    ["objective","metric","learning_rate","num_leaves","feature_fraction","feature_fraction_bynode","bagging_fraction","lambda_l2"]
    if k in params_oof
})

# --- OOF用の重み取得関数（位置参照固定）をループ前に定義 ---
def _ensure_oof_train_weight():
    import numpy as np
    global train_weight

    n = len(train)
    # 未定義 or 長さ不一致なら学習時と同条件で再計算
    if ('train_weight' not in globals()) or (getattr(train_weight, '__len__', lambda: -1)() != n):
        y_all = train['y_win'].astype('int8').values
        _w_p  = _compute_payout_weight(train, y_all, bet_unit=BET_UNIT, alpha=PAYOUT_ALPHA, cap_mult=PAYOUT_CAPMULT)
        _w_r  = _compute_recency_weight(train, alpha=RECENCY_ALPHA)
        _w_g  = pd.to_numeric(train.get("__sample_weight_stage_c__", 1.0), errors="coerce").fillna(1.0).astype("float32").values
        w = (_w_p * _w_r * _w_g).astype("float32")
        if os.environ.get("RENORM_WEIGHTS", "1") == "1":
            m = float(w.mean())
            if m > 0:
                w = (w / m).astype("float32")
        train_weight = w  # グローバル更新

    # どんな型でも ndarray に落として「位置参照」を保証
    return np.asarray(train_weight, dtype="float32")

# ---- OOF ループ ----
for fold, (tr_idx, va_idx) in enumerate(splitter.split(train), 1):
    print(f"  - Fold {fold}/{N_SPLITS}")

    X_tr = train.iloc[tr_idx][FEATURE_COLS]
    y_tr = train.iloc[tr_idx][TARGET].astype('int8').values
    X_va = train.iloc[va_idx][FEATURE_COLS]
    y_va = train.iloc[va_idx][TARGET].astype('int8').values

    # 追加（整列ガード。ズレてたら即停止）
    assert np.array_equal(X_tr.index.values, train.iloc[tr_idx].index.values), \
        "[ERR] OOF: X_tr と train[tr_idx] のindex不一致"
    assert np.array_equal(X_va.index.values, train.iloc[va_idx].index.values), \
        "[ERR] OOF: X_va と train[va_idx] のindex不一致"

    # 重みを位置参照で取得
    _w_all = _ensure_oof_train_weight()
    # Title: OOFでも本体と同じサンプル重みを適用
    # Description: 較正用logitの分布を本体学習と一致させ、T学習のズレを防ぐ
    # タイトル: OOF用サンプル重み取得の loc→iloc 修正
    # 説明: TimeSeriesSplit が返す tr_idx/va_idx は位置インデックス。iloc で取得し KeyError を回避
    _w_tr = train["__sample_weight_stage_c__"].to_numpy()[tr_idx]   # ← 位置参照
    _w_va = np.ones(len(va_idx), dtype="float32")                   # 監視は等重みでOK（揃えたいなら同列でも可）

    dtr = lgb.Dataset(train.iloc[tr_idx][FEATURES], label=train.iloc[tr_idx]["y_win"], weight=_w_tr)
    dva = lgb.Dataset(train.iloc[va_idx][FEATURES], label=train.iloc[va_idx]["y_win"], weight=_w_va)



    fold_model = lgb.train(
        params_oof, d_tr,
        num_boost_round=5000,
        valid_sets=[d_va],
        valid_names=['valid_fold'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(period=0),
        ]
    )

    oof_logit[va_idx] = fold_model.predict(
        X_va, num_iteration=fold_model.best_iteration, raw_score=True
    )
    best_iters.append(int(fold_model.best_iteration or 100))

    del d_tr, d_va, X_tr, X_va, y_tr, y_va, fold_model
    gc.collect()

print(f"[INFO] OOF（raw_logit）完了: shape={oof_logit.shape}  / best_iter(median/mean)={np.median(best_iters):.0f}/{np.mean(best_iters):.1f}")


# タイトル: 動的T(g)の学習（重み付き）呼び出し
# タイトル: 動的T(g)の学習（重み付き）
# 説明: OOFの「有効行」に合わせ、Cell-4.7 で作った学習重みで NLL を最小化
m_use   = np.isfinite(raw_logit_oof) & train["y_win"].notna()
w_calib = train.loc[m_use, "__sample_weight_stage_c__"].astype("float32").values  # ← ブールマスクなので loc でOK

theta = _learn_dynamic_T_from_oof(
    raw_logit_oof[m_use],
    train.loc[m_use, "y_win"].astype("int8").values,
    train.loc[m_use, :],     # グループ計算用に同じ行を渡す
    weight=w_calib,          # ★ここが“1行”の肝
    l2=1e-4,
    normalize_weight=True
)
a0, a1, a2 = map(float, theta)
print(f"[INFO] 学習した動的温度パラメータ（weighted）: a0={a0:.4f}, a1={a1:.4f}, a2={a2:.4f}")



# ---- 学習済み最終モデルの raw_logit を取得（Valid/Test） -------------
model, best_iter = _get_model_and_best_iter()
valid_logit = model.predict(valid[FEATURE_COLS], num_iteration=best_iter, raw_score=True)
test_logit  = model.predict(test[FEATURE_COLS],  num_iteration=best_iter, raw_score=True)

# ---- 温度：静的T（従来）を廃止 → 「動的温度 T(g)」を学習・適用 -------
gid_tr = _group_ids(train, ("date","race_code"))
y_tr   = train[TARGET].astype('int8').values

#T_params = _learn_dynamic_T_from_oof(oof_logit, y_tr, train)  # (a0,a1,a2)
# パッチ版で上書き学習（セル内のみ反映）
T_params = _learn_dynamic_T_from_oof_patch(raw_logit_oof, (y_train>0), train)  # ← 新

print(f"[INFO] 学習した動的温度パラメータ: a0={T_params[0]:.4f}, a1={T_params[1]:.4f}, a2={T_params[2]:.4f}  "
      f"(T_min={T_MIN:.2f}, T_max={T_MAX:.2f})")

# ---- softmax（T=1 と 動的T(g)）で確率化 ------------------------------
def _probs_from_logit(df, raw_logit, T):
    gid = _group_ids(df, ("date","race_code"))
    return _softmax_by_group(raw_logit.astype('float64'), gid, T=T)

# raw（T=1.0）
train_prob_raw = _probs_from_logit(train, oof_logit,     T=1.0)
valid_prob_raw = _probs_from_logit(valid, valid_logit,  T=1.0)
test_prob_raw  = _probs_from_logit(test,  test_logit,   T=1.0)

# calibrated（動的T(g)）
train_prob_cal = _probs_from_logit_dynamicT(train, oof_logit,    T_params)
valid_prob_cal = _probs_from_logit_dynamicT(valid, valid_logit,  T_params)
test_prob_cal  = _probs_from_logit_dynamicT(test,  test_logit,   T_params)

# ---- レース内合計チェック -------------------------------------------
def _sum_by_race(df, p):
    q = df.assign(_p=p).groupby(["date","race_code"], sort=False)["_p"].sum().quantile([0.5,0.9,1.0]).values
    return q
q_tr = _sum_by_race(train, train_prob_cal)
q_va = _sum_by_race(valid, valid_prob_cal)
q_te = _sum_by_race(test,  test_prob_cal)
print(f"[DBG] Train(OOF) 予測合計/レース: p50={q_tr[0]:.3f}, p90={q_tr[1]:.3f}, max={q_tr[2]:.3f}")
print(f"[DBG] Valid      予測合計/レース: p50={q_va[0]:.3f}, p90={q_va[1]:.3f}, max={q_va[2]:.3f}")
print(f"[DBG] Test       予測合計/レース: p50={q_te[0]:.3f}, p90={q_te[1]:.3f}, max={q_te[2]:.3f}")

# 参考：学習時のレース別T(g)の概要
_gid, _hhi, _logn = _group_metrics_for_T(train)
_Tg = _T_from_params(T_params, _hhi, _logn)
print(f"[INFO] 動的T(g) 概況: median={np.median(_Tg):.3f}, p10={np.percentile(_Tg,10):.3f}, p90={np.percentile(_Tg,90):.3f}")

# ---- メトリクス（LogLoss/AUC/Brier） --------------------------------
def _metrics(y, p):
    return {
        "logloss": log_loss(y, p),
        "auc":     roc_auc_score(y, p),
        "brier":   brier_score_loss(y, p)
    }

m_train_raw = _metrics(train[TARGET].values, train_prob_raw)
m_valid_raw = _metrics(valid[TARGET].values, valid_prob_raw)
m_test_raw  = _metrics(test[TARGET].values,  test_prob_raw)

m_train_cal = _metrics(train[TARGET].values, train_prob_cal)
m_valid_cal = _metrics(valid[TARGET].values, valid_prob_cal)
m_test_cal  = _metrics(test[TARGET].values,  test_prob_cal)

def _print_table(tag, M):
    print(tag)
    print("="*40)
    print("Split      | LogLoss |   AUC   |  Brier")
    print("-"*40)
    for name, m in M:
        print(f"{name:<10} | {m['logloss']:.5f} | {m['auc']:.5f} | {m['brier']:.5f}")
    print("="*40)

_print_table("\n[INFO] モデル評価（softmax後 / T=1.0）", [
    ("Train(OOF)", m_train_raw),
    ("Valid",      m_valid_raw),
    ("Test",       m_test_raw),
])
_print_table("\n[INFO] モデル評価（温度較正後 / 動的T(g)）", [
    ("Train(OOF)", m_train_cal),
    ("Valid",      m_valid_cal),
    ("Test",       m_test_cal),
])

# ---- 出力列を統一：pred_win_prob を『較正後 softmax 確率』に確定 ----
for df_ in (train, valid, test):
    if 'pred_win_prob' in df_.columns:
        df_['pred_win_prob_raw_backup'] = df_['pred_win_prob'].values

train['pred_win_prob'] = train_prob_cal.astype('float32')
valid['pred_win_prob'] = valid_prob_cal.astype('float32')
test['pred_win_prob']  = test_prob_cal.astype('float32')

# ---- 温度をメタ保存（動的パラメータを保存） --------------------------
try:
    meta_path = globals().get('STAGE_C_META_PATH', os.path.join(ART_DIR_C, "JRA_stage_c_meta.json"))
    if meta_path:
        _meta = {}
        if os.path.exists(meta_path):
            with open(meta_path, 'r') as f: _meta = json.load(f)
        _meta['temperature_softmax'] = {
            "mode": "dynamic_sigmoid_linear",
            "T_min": float(T_MIN),
            "T_max": float(T_MAX),
            "params": [float(T_params[0]), float(T_params[1]), float(T_params[2])],
            "note": "T(g)=T_min+(T_max-T_min)*sigmoid(a0 + a1*HHI + a2*logN); HHI from TSR."
        }
        with open(meta_path, 'w') as f: json.dump(_meta, f, indent=2)
        print(f"[INFO] 動的温度メタを保存: {meta_path}")
except Exception as e:
    print(f"[WARN] 温度メタ保存に失敗: {e}")

print("\n[NOTE] 以降のデシル/TSR/Top-k評価は『pred_win_prob（= 動的T(g)較正済）』を使用します。")

# ---- デシル分析 ----
def decile_table(df_src, pcol, payout_col, title):
    df = df_src[[pcol, payout_col]].copy()
    r = df[pcol].rank(method="first")
    try:
        dec = pd.qcut(r, 10, labels=False, duplicates='drop') + 1
    except ValueError:
        dec = pd.qcut(r, 5, labels=False, duplicates='drop') + 1
        dec = dec * 2
    df['decile'] = dec.astype(int)
    agg = df.groupby('decile').agg(
        count=(pcol,'count'),
        mean_prob=(pcol,'mean'),
        hit_rate=(payout_col, lambda x: (x>0).mean()*100),
        roi=(payout_col, lambda x: x.sum()/(len(x)*BET_UNIT)*100 if len(x)>0 else 0)
    ).sort_index(ascending=False)
    print(f"\n[Decile] {title} :\n{agg.to_string(float_format='%.2f')}")
    return agg

# ---- 10% 刻み確率bin ----
def prob_bins(df_src, pcol, payout_col, title):
    bins = np.linspace(0,1,11)
    cut = pd.cut(df_src[pcol], bins, include_lowest=True)
    agg = df_src.groupby(cut).agg(
        count=(pcol,'count'),
        avg_pred=(pcol,'mean'),
        emp_rate=(payout_col, lambda x: (x>0).mean()*100),
        roi=(payout_col, lambda x: x.sum()/(len(x)*BET_UNIT)*100 if len(x)>0 else 0)
    )
    print(f"\n[ProbBins] {title} (10%刻み)\n{agg.to_string(float_format='%.2f')}")
    return agg

# ---- Top-k per Race ROI ----
def roi_topk_per_race(df_src, pcol, payout_col, k=1):
    g = df_src.sort_values(pcol, ascending=False).groupby(["date","race_code"], sort=False)
    topk = g.head(k)
    n = len(topk)
    if n == 0:
        return 0.0, 0
    roi = topk[payout_col].sum()/(n*BET_UNIT)*100
    return float(roi), int(n)

# === TSR（trifecta_support_rate）× Pred の“バリュー”可視化（追加関数） ===
def _normalize_tsr_prob(df, tsr_col="trifecta_support_rate", group_keys=("date","race_code")):
    g = df[list(group_keys)].apply(tuple, axis=1)
    s = pd.to_numeric(df[tsr_col], errors="coerce").fillna(0.0).astype("float32")
    denom = s.groupby(g).transform("sum").replace(0, np.nan)
    tsr_prob = (s / denom).fillna(0.0).astype("float32")
    return tsr_prob

def decile_table_tsr_ex(df_src, pcol, payout_col, title, tsr_col="trifecta_support_rate", gap_ths=(0.0, 0.02, 0.05)):
    cols_need = [pcol, payout_col, tsr_col, "date", "race_code"]
    df = df_src[cols_need].copy()

    # レース内TSR正規化（確率スケール：合計1）
    gkey = df[["date","race_code"]].apply(tuple, axis=1)
    s = pd.to_numeric(df[tsr_col], errors="coerce").fillna(0.0).astype("float32")
    denom = s.groupby(gkey).transform("sum").replace(0, np.nan)
    df["tsr_prob"] = (s / denom).fillna(0.0).astype("float32")

    r = df[pcol].rank(method="first")
    try:
        dec = pd.qcut(r, 10, labels=False, duplicates='drop') + 1
    except ValueError:
        dec = pd.qcut(r, 5, labels=False, duplicates='drop') + 1
        dec = dec * 2
    df["decile"] = dec.astype(int)

    df["value_gap"] = (df[pcol] - df["tsr_prob"]).astype("float32")

    agg = df.groupby("decile").agg(
        count=("date","count"),
        mean_pred=(pcol,"mean"),
        mean_tsr_prob=("tsr_prob","mean"),
        mean_value_gap=("value_gap","mean"),
        share_value_pos=("value_gap", lambda x: (x>0).mean()*100),
    ).sort_index(ascending=True)

    for th in gap_ths:
        bp = int(round(th * 10000))
        lab_roi  = f"roi_value_gap>={bp}bp"
        lab_bets = f"bets_gap>={bp}bp"
        mask = (df["value_gap"] >= th)
        grp  = df[mask].groupby("decile")
        roi_by_dec  = grp[payout_col].apply(lambda x: (x.sum() / (len(x)*BET_UNIT) * 100) if len(x)>0 else 0.0)
        bets_by_dec = grp.size()
        agg = agg.join(roi_by_dec.rename(lab_roi),  how="left")
        agg = agg.join(bets_by_dec.rename(lab_bets), how="left")

    agg = agg.fillna(0.0).sort_index(ascending=False)

    print(f"\n[TSR×Pred] {title}（デシル別・市場比較）:")
    print(agg.to_string(float_format="%.2f"))

    # 全体での gap フィルタ回収率
    for th in gap_ths:
        sub = df[df["value_gap"] >= th]
        bets = len(sub)
        roi = sub[payout_col].sum()/(bets*BET_UNIT)*100 if bets>0 else 0.0
        print(f"   - 全体: gap≥{th:.3f} → ROI={roi:.2f}%  bets={bets}")

    def _roi_topk_value(df_sub, k, th):
        g = df_sub[df_sub["value_gap"]>=th].sort_values(pcol, ascending=False).groupby(["date","race_code"], sort=False)
        pick = g.head(k)
        n = len(pick)
        roi = pick[payout_col].sum()/(n*BET_UNIT)*100 if n>0 else 0.0
        return roi, n

    for th in gap_ths:
        for k in (1, 2):
            r_, n_ = _roi_topk_value(df, k, th)
            print(f"   - [Value gap≥{th:.3f}] Top{k}/race: ROI={r_:.2f}%  bets={n_}")

    return agg


va_dec = decile_table(valid, "pred_win_prob", "win_payout", "Valid")
te_dec = decile_table(test,  "pred_win_prob", "win_payout", "Test")
va_bins = prob_bins(valid, "pred_win_prob", "win_payout", "Valid")
te_bins = prob_bins(test,  "pred_win_prob", "win_payout", "Test")

r1_v, n1_v = roi_topk_per_race(valid, "pred_win_prob", "win_payout", 1)
r2_v, n2_v = roi_topk_per_race(valid, "pred_win_prob", "win_payout", 2)
r1_t, n1_t = roi_topk_per_race(test,  "pred_win_prob", "win_payout", 1)
r2_t, n2_t = roi_topk_per_race(test,  "pred_win_prob", "win_payout", 2)
print(f"\n-- Top-k per Race --\n[Top1/race] Valid ROI={r1_v:.2f}% bets={n1_v} | [Top2/race] ROI={r2_v:.2f}% bets={n2_v}")
print(f"[Top1/race] Test ROI={r1_t:.2f}% bets={n1_t} | [Top2/race] ROI={r2_t:.2f}% bets={n2_t}")

print("\n=== TSR add-on (model vs market) ===")
_ = decile_table_tsr_ex(valid, "pred_win_prob", "win_payout", title="Valid", tsr_col="trifecta_support_rate", gap_ths=(0.0, 0.02, 0.05))
_ = decile_table_tsr_ex(test,  "pred_win_prob", "win_payout", title="Test",  tsr_col="trifecta_support_rate", gap_ths=(0.0, 0.02, 0.05))

# ------------------------
# Cell-6.5: リークっぽさの自動検知（任意停止）
# ------------------------
if STRICT_LEAK_GUARD:
    try:
        top_dec_va = float(va_dec.iloc[0]['hit_rate']) if not va_dec.empty else 0.0
        top_dec_te = float(te_dec.iloc[0]['hit_rate']) if not te_dec.empty else 0.0
        if (top_dec_va > 40.0 and top_dec_te > 40.0):
            raise RuntimeError(
                f"[LEAK-GUARD] Topデシルの的中率が異常に高い可能性があります (Valid={top_dec_va:.1f}%, Test={top_dec_te:.1f}%).\n"
                f"Stage-A/オッズ由来の確定情報（確定人気/最終オッズ）や配当列が混入していないか確認してください。"
            )
    except Exception as e:
        raise

# ------------------------
# Cell-7: 成果物保存
# ------------------------
print("\n--- Cell-7: 成果物を書き出します ---")
# 予測ファイル（valid+test）
try:
    out = pd.concat([
        valid[MERGE_KEYS + ["pred_win_prob"]].assign(split="valid"),
        test[MERGE_KEYS + ["pred_win_prob"]].assign(split="test")
    ], ignore_index=True)
    out_path = os.path.join(ART_DIR_C, "JRA_stage_c_predictions.parquet")
    out.to_parquet(out_path, index=False, engine="pyarrow", compression="zstd")
    print(f"   - 予測書き出し:  {out_path}  rows= {len(out):,}")
except Exception as e:
    print(f"   - [WARN] 予測の書き出しに失敗: {e}")

# メタ（既存を保持しつつ必要項目だけ上書き）
meta_path = os.path.join(ART_DIR_C, "JRA_stage_c_meta.json")

# 既存メタを読み込み（無ければ空dict）
_meta = {}
if os.path.exists(meta_path):
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            _meta = json.load(f)
    except Exception:
        _meta = {}

# === 追加：Stage-C のカテゴリ全集合を保存（today推論で厳密一致させる） ===
cat_categories_c = {}
for c in cat_cols_final:
    if c in train.columns:
        # ここは “train のユニーク” ではなく “確定 categories”
        cats = list(train[c].cat.categories.astype("string"))
        # 念のため順序安定化（任意）
        cat_categories_c[c] = list(cats)

_meta["cat_categories_c"] = cat_categories_c

# --- Soft-Finish のメタを最終メタに合流 ---
sf_meta = {
    "model_path": os.path.join(ART_DIR_C, "JRA_stage_c_softfinish_model.pkl"),
    "num_boost_round": int(num_boost_round_sf),
    "feature_cols": FEAT_COLS_SF,
    "categorical_cols": cat_used_sf,
    "cat_categories": {c: CAT_MAP_FINAL[c] for c in cat_used_sf},
    "winsor": {"q_low": WINSOR_Q_LOW, "q_high": WINSOR_Q_HIGH, "lo": float(lo), "hi": float(hi)}
}
_meta["soft_finish"] = sf_meta


# 今回更新するキーだけ上書き
_meta.update({
    "seed": SEED,
    "train_range": [str(TRAIN_START_DATE.date()), str(TRAIN_END_DATE.date())],
    "valid_range": [str(VALID_START_DATE.date()), str(VALID_END_DATE.date())],
    "test_range":  [str(TEST_START_DATE.date()),  str(TEST_END_DATE.date())],
    "features": FEATURES,
    "categorical_features": cat_cols_final,
    "best_iter": int(best_iter),
    "cov": probe,
    "model_path": STAGE_C_MODEL_PATH,  # ← 追加
})

# 保存（既存の soft_finish / temperature_softmax は保持される）
with open(meta_path, "w", encoding="utf-8") as f:
    json.dump(_meta, f, ensure_ascii=False, indent=2)
print(f"   - META 保存:  {meta_path}")


print("\n✅ Stage-C 完了（単勝的中率モデル / 厳格マージ＆停止ガード）")

# --- Stage-C: pred_win_prob を必ず用意するためのドロップイン・パッチ（※原文そのまま残置） ---
# ※ 上で Train/Valid/Test すべてに pred_win_prob を作成済みなので、このパッチは基本的に何もしません
#    （列が無い場合の保険・値域チェック・DBG出力のみ動作）
import re, numpy as np
import pandas as pd
import lightgbm as lgb

# 1) 使う特徴量リストを特定（FEATURESがなければ代替）
if 'FEATURES' in globals():
    FEATS = FEATURES
elif 'feature_cols' in globals():
    FEATS = feature_cols
else:
    _exclude = set([
        # キー・目的・明確なリーク列
        'date','race_code','horse_number','race_number',
        'win_flag','win_payout','place_payout',
        'target','target_raw','target_capped',
        'pred_ev_stage_b_oof','pred_ev_stage_b_pred','pred_ev_stage_b_any',
    ])
    # 数値主体で暫定抽出
    FEATS = [c for c in valid.columns if c not in _exclude and pd.api.types.is_numeric_dtype(valid[c])]

# 2) Booster を取得
_booster = None
for cand in ('model','gbm','gbm_final','clf'):
    if cand in globals() and isinstance(globals()[cand], lgb.Booster):
        _booster = globals()[cand]; break
if _booster is None:
    raise RuntimeError("LightGBM Booster が見つかりません。学習セルの実行順を確認してください。")

_best_iter = getattr(_booster, 'best_iteration', None)

def _ensure_pred_col(df, name='pred_win_prob'):
    """name列がなければ予測して作成。存在しても値域を点検して補正。"""
    if name not in df.columns:
        # 既存の“それっぽい”列があれば流用（最優先：pred_win_prob）
        cand = None
        for c in df.columns:
            if c == 'pred_win_prob':
                cand = c; break
        # “pred_*prob*” のような列名があれば候補にする（Stage-Aの prob_gap_* は除外）
        if cand is None:
            pats = (r'^pred.*prob', r'pred.*proba', r'^proba$', r'^pred$')
            bad  = ('prob_gap', 'pred_prob_stage_a')  # 明確に別物
            for c in df.columns:
                if any(re.search(p, c) for p in pats) and not any(b in c for b in bad):
                    cand = c; break

        if cand is not None:
            s = pd.to_numeric(df[cand], errors='coerce')
            df[name] = s
        else:
            # 予測して作る（通常は到達しない）
            X = df[FEATS].copy()
            _num_cols = [c for c in FEATS if pd.api.types.is_float_dtype(X[c])]
            if _num_cols:
                X.loc[:, _num_cols] = X.loc[:, _num_cols].astype('float32', copy=False)
            p = _booster.predict(X, num_iteration=_best_iter, raw_score=False)
            df[name] = pd.Series(p, index=df.index)

    # 値域・形式チェック（logit っぽければシグモイド）
    p = pd.to_numeric(df[name], errors='coerce').astype('float64')
    if (p.min() < 0.0) or (p.max() > 1.0):
        if p.min() < -5 or p.max() > 5:
            p = 1.0 / (1.0 + np.exp(-p))
        p = np.clip(p, 0.0, 1.0)
    df[name] = p.astype('float32')

# 3) 各分割に適用（上流で作成済みなら値域チェックのみ）
for _df in (train, valid, test):
    _ensure_pred_col(_df, name='pred_win_prob')

# 4) 追加の健全性チェック（任意）
for _name, _df in [('Train', train), ('Valid', valid), ('Test', test)]:
    p = _df['pred_win_prob'].to_numpy(dtype='float32')
    assert np.isfinite(p).all(), f"{_name}: pred_win_prob に NaN/Inf が含まれます。"
    if not (0.0 <= p.min() <= p.max() <= 1.0):
        raise RuntimeError(f"{_name}: pred_win_prob の値域が [0,1] 外です。作成手順を確認してください。")
    # レース内合計の参考値
    if {'date','race_code'}.issubset(_df.columns):
        s = _df.groupby(['date','race_code'], sort=False)['pred_win_prob'].sum()
        print(f"[DBG] {_name} 予測合計/レース: p50={s.median():.3f}, p90={s.quantile(.9):.3f}, max={s.max():.3f}")


def top1_margin_report(df, pcol="pred_win_prob", payout_col="win_payout",
                       bins=(0.00, 0.02, 0.05, 0.10, 0.20, 0.30, 1.01), name="Valid"):
    # レース内で上位2頭を抽出
    g = df.sort_values(pcol, ascending=False).groupby(["date","race_code"], sort=False)
    top2 = g.head(2).copy()
    top2["__rank"] = top2.groupby(["date","race_code"])[pcol].rank(method="first", ascending=False)

    p1 = top2[top2["__rank"]==1].copy()
    p2 = (top2[top2["__rank"]==2][["date","race_code", pcol]]
            .rename(columns={pcol:"p2"}))
    p1 = p1.merge(p2, on=["date","race_code"], how="left")
    p1["p2"] = p1["p2"].fillna(0.0)
    p1["margin"] = (p1[pcol] - p1["p2"]).astype("float32")

    # bin化（右開区間にするため right=False）
    import numpy as np, pandas as pd
    bins = list(bins)
    labels = [f"[{bins[i]:.2f},{bins[i+1]:.2f})" for i in range(len(bins)-1)]
    p1["margin_bin"] = pd.cut(p1["margin"], bins=bins, labels=labels, right=False, include_lowest=True)

    def _roi(x):
        n = len(x)
        return 0.0 if n==0 else x.sum()/(n*BET_UNIT)*100.0

    out = (p1.groupby("margin_bin")
             .agg(races=("date","count"),
                  top1_avg_p=(pcol, "mean"),
                  margin_avg=("margin","mean"),
                  top1_hit_rate=(payout_col, lambda x: (x>0).mean()*100.0),
                  top1_roi=(payout_col, _roi))
             .reset_index())

    print(f"\n[Top1 成績 by マージン] {name}")
    print(out.to_string(index=False, float_format="%.2f"))
    return out

# 実行例
_ = top1_margin_report(valid, name="Valid")
_ = top1_margin_report(test,  name="Test")


# --- Cell-X2: 特徴量重要度 Top50（Stage-C / LightGBM） ---
import os, json, re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 保存先（JRA06固定）
STAGE_C_ART_DIR = "/content/drive/My Drive/Colab/gpt/JRA06/artifacts_stage_c"
os.makedirs(STAGE_C_ART_DIR, exist_ok=True)
FI_CSV_PATH = os.path.join(STAGE_C_ART_DIR, "JRA_stage_c_feature_importance.csv")

# 1) Booster を取得
booster = None
for cand in ["model", "gbm_final", "bst"]:
    if cand in globals() and hasattr(globals()[cand], "feature_importance"):
        booster = globals()[cand]
        print(f"[INFO] Booster 検出: {cand}")
        break
if booster is None:
    raise RuntimeError("LightGBM の学習済みモデル（model / gbm_final / bst）が見つかりません。学習セルの直後で実行してください。")

# 2) 重要度をDataFrame化（gain / split 両方）
feat_names = booster.feature_name()
fi_gain  = booster.feature_importance(importance_type="gain")
fi_split = booster.feature_importance(importance_type="split")
fi = pd.DataFrame({"feature": feat_names, "gain": fi_gain, "split": fi_split})
fi["gain_norm"] = fi["gain"] / (fi["gain"].sum() + 1e-12)
fi = fi.sort_values("gain", ascending=False).reset_index(drop=True)

# 3) 表示：Top 50
topn = 50
print("\n[Feature Importance] Top 50 by GAIN")
disp = fi.head(topn).copy()
# 見やすいように丸め
disp["gain_norm"] = disp["gain_norm"].map(lambda x: f"{x:.4f}")
disp["gain"] = disp["gain"].map(lambda x: f"{x:.1f}")
print(disp.to_string(index=False))

# 4) CSV保存（上位200も併せて保存）
fi.head(200).to_csv(FI_CSV_PATH, index=False)
print(f"\n[SAVE] 重要度CSVを書き出しました: {FI_CSV_PATH}")

# 5) 可視化：水平バー（Top 50）
plt.figure(figsize=(10, max(6, int(0.22*len(disp)))))
plt.barh(disp["feature"][::-1], disp["gain"].astype(float)[::-1])
plt.xlabel("gain")
plt.ylabel("feature")
plt.title("Stage-C Feature Importance (Top 50 by gain)")
plt.tight_layout()
plt.show()

# 6) 簡易リークスキャン（疑わしいトークンが上位にいないかを目視補助）
suspect_tokens = [
    "odds", "payout", "trifecta_support_rate", "final", "確定", "popular", "popularity"
]
sus = [f for f in fi.head(topn)["feature"] if any(t in f.lower() for t in suspect_tokens)]
if sus:
    print("\n[LEAK-SCAN] Top50内で疑わしい特徴名（手動点検を推奨）:")
    for s in sus:
        print("  -", s)
else:
    print("\n[LEAK-SCAN] Top50内に典型的なリークトークンは見当たりません。")
